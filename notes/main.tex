\documentclass{article} % For LaTeX2e
\usepackage{iclr2021_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
  
\usepackage{hyperref}
\usepackage{url}

\usepackage{algorithm, algpseudocode}
% \usepackage{algorithmic}
% \usepackage{algorithm2e}
\usepackage{relsize} %SEB: feel free to revert, I think the font looked a bit too big otherwise
\newcommand{\algname}[1]{{\sf\green\relscale{0.90}#1}\xspace}
\newcommand{\algnameS}[1]{{\sf\green\relscale{0.90}#1}\xspace}
\newcommand{\dataname}[1]{{\tt\color{blue}#1}\xspace}
\newcommand{\sybg}[1]{{\sf\green#1}\xspace}  % new added symbol green
\newcommand{\sybr}[1]{{\sf\red#1}\xspace}  % new added symbol green

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{makecell}
\newcommand{\eqdef}{\coloneqq}
\usepackage{ulem}

\usepackage{subcaption}

\usepackage{nicefrac}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\C}{\mathcal{C}}
%%%%% PETER ADDED MACROS %%%%%
\usepackage{tcolorbox}
\usepackage{pifont}
% \definecolor{mydarkgreen}{RGB}{5,81,23}
\definecolor{mydarkgreen}{RGB}{39,130,67}
\definecolor{mydarkred}{RGB}{192,25,25}
\newcommand{\green}{\color{mydarkgreen}}
\newcommand{\red}{\color{mydarkred}}
\newcommand{\cmark}{\green\ding{51}}%
\newcommand{\xmark}{\red\ding{55}}%
\usepackage{amssymb}
\usepackage{xspace}
\usepackage{bm}

\usepackage[colorinlistoftodos,bordercolor=orange,backgroundcolor=orange!20,linecolor=orange,textsize=scriptsize]{todonotes}
\newcommand{\kai}[1]{\todo[inline]{{\textbf{Kai:} \emph{#1}}}}

\title{A Unified Theory of Error Feedback and Variance Reduction for Non-Convex Optimization}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Kai Yi\\
   King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia\\
   \texttt{kai.yi@kaust.edu.sa}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
% \newcommand{\h0}{{\green $h^{0}$}}
% \newcommand{\ht}{{\green $h^{t}$}}
% \newcommand{\mt}{{\red $m^{t}$}}
% \DeclareMathOperator*{\argmin}{arg\,min}
% \DeclareMathOperator*{\minimize}{\mathrm{minimize}}
\newcommand{\sqnorm}[1]{\left\| #1 \right\|^2}
\newcommand{\Exp}[1]{\mathbb{E}\!\left[ #1 \right]}
\newcommand{\oma}{\omega_{\mathrm{av}}}
\newcommand{\gv}{v} 

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\begin{document}
\maketitle


\begin{abstract}  
In this project, we first extend EF-BV~\cite{condat2022ef} to non-convex setting and consider the special case under PL inequality. Then we extend EF-BV to pratical federated learning scenarios including variance reduction, partial participation, bidirectional compression.  
\end{abstract}

\tableofcontents
\newpage

\section{Background}
\subsection{Problem Definition}
We consider the standard federated optimization problem

\begin{equation}
   \min_{x\in\mathbb{R}^d} f(x) = \frac{1}{n}\sum_{i=1}^n f_i (x),
\end{equation}

where $n$ is the number of clients. $f_i$ is the local optimization function at client $i$ of the form 

\begin{equation}
   f_i(x) = \frac{1}{m} \sum_{j=1}^m f_{ij} (x),
\end{equation}

where $m$ is the number of datapoints at client $i$. 

\subsection{General Compressors}
Based on bias-variance decomposition of the compression error (\algname{EF-BV}~\cite{condat2022ef}, Sec 2.3); for every $x\in \R^d$,

\begin{equation}
   \mathbb{E}\big[\|\mathcal{C}(x)-x\|^2\big] = {\underbrace{\big\| \mathbb{E}[\mathcal{C}(x)]-x\big\|}_{\text{bias}}}^2 + \underbrace{\mathbb{E}\Big[\big\|\mathcal{C}(x)-\mathbb{E}[\mathcal{C}(x)]\big\|^2\Big]}_{\text{variance}},\label{eqbiva}
   \end{equation}

where the two terms at the right hand side satisfies

\begin{equation}\label{def:compressors}
   \begin{aligned}
      \|\mathbb{E}[\mathcal{C}(x)]-x\| &\leq \eta\|x\|,\\
      \mathbb{E}\left[\|\mathcal{C}(x)-\mathbb{E}[\mathcal{C}(x)]\|^{2}\right] &\leq \omega\|x\|^{2}.
   \end{aligned}
\end{equation}

$\eta$ and $\omega$ is interpreted as the relative bias and variance controllers of the general compressor. Unbiased compressors and contractive biased compressors are all special case of this general compressor. More details could refer to~\algname{EF-BV}~\cite{condat2022ef}, Sec 2.3.

\subsection{Average Variance of Compressors}
Given $n$ compressors $\C_i$, the average relative variance~\cite{condat2022ef} $\omega_{av}$ is defined as: for every $x_i\in \R^d$, 

\begin{equation}
   \left.\mathbb{E}\left[\left\|\frac{1}{n} \sum_{i=1}^{n}\left(\mathcal{C}_{i}\left(x_{i}\right)-\mathbb{E}\left[\mathcal{C}_{i}\left(x_{i}\right)\right]\right)\right\|^{2}\right]\right] \leq \frac{\omega_{\mathrm{av}}}{n} \sum_{i=1}^{n}\left\|x_{i}\right\|^{2}.
   \end{equation}

The property of this bound from \algname{EF-BF}: $\omega_{\mathrm{av}}\leq \omega$ and can be much smaller than $\omega$. When $\C_i$ are mutually independent, $\omega_{\mathrm{av}} = \omega / n$.

% \kai{TODO: understand the independence here.}

\subsection{EF-BV}
The key part of \algname{EF-BV}~\cite{condat2022ef} is that the gradient estimator is updated By

\begin{equation}
   g_i^t = h_i^t + \sybg{\nu} \C(\nabla f_i (x^t) - h_i^t)
\end{equation}

where the control variates are updated as 

\begin{equation}
   h_i^{t+1} = h_i^t + \sybr{\lambda} \C(\nabla f_i (x^t) - h_i^t).
\end{equation}

More details could refer to \algname{EF-BV}~\cite{condat2022ef} Sec. 3.

\section{Convergence Analysis for Non-Convex Setting}
   \subsection{Assumptions}
   \begin{assumption}[Smoothness and Lower Bound]\label{assump:smooth}
      Every $f_i$ is $L_i$-smooth and $f$ is $L$-smooth. $f^{\inf}\eqdef \inf_{x\in \R^d} f(x) > -\infty$.
   \end{assumption}

   Using Jensen's inequality, we have $L\leq \frac{1}{n}\sum_{i}L_i$. Let $\tilde{L}\eqdef (\frac{1}{n}\sum_{i=1}^2)^{1/2}$. Using the arithmetic-quadratic mean inequality, $\frac{1}{n}\sum_{i}L_i \leq \tilde{L}$.

   \begin{assumption}[Polyak-Åojasiewicz Condition]\label{assump:pl}
      
      There exists $\mu>0$ such that 

      \begin{equation}
         f(x) - f^{\inf} \leq \frac{1}{2\mu} \| \nabla f(x)\|^2, \quad \forall x\in\R^d
      \end{equation}
   \end{assumption}

   \subsection{Main Theorem}
   \begin{theorem}[Linear Convergence in the non-convex setting]\label{thm:main}
      Suppose Assumption~\ref{assump:smooth} is satisfied. Suppose $\nu\in (0, 1], \lambda\in (0,1]$, choosing the stepsize 
      
      $$0< \gamma \leq \frac{1}{L + \tilde{L}\sqrt{\frac{r_{\mathrm{av}}}{r}}\frac{1}{s}}.$$

      For every $t\geq 0$, define the Lyapunov function

      \begin{equation}
         \Psi^{t}:=f\left(x^{t}\right)-f^{\inf}+\frac{\gamma}{2 \theta} \frac{1}{n} \sum_{i=1}^{n}\left\|\nabla f_{i}\left(x^{t}\right)-h_{i}^{t}\right\|^{2}
         \end{equation}
      
      Fix $T\geq 1$ and let $\hat{x}^T$ be chosen from the iterates $x^0, x^1, \cdots, x^{T-1}$ uniformly at random. Then 

      \begin{equation}
         \mathbb{E}\left[\left\|\nabla f\left(\hat{x}^{T}\right)\right\|^{2}\right] \leq \frac{2\left(f\left(x^{0}\right)-f^{\inf}\right)}{\gamma T}+\frac{\mathbb{E}\left[G^{0}\right]}{\theta T},
         \end{equation}

      where $G^0=\frac{1}{n}\sum_{i=1}^n \sqnorm{\nabla f_i(x^0) - h_i^0}$.
   \end{theorem}

   Similar to \algname{EF-BV}, we define the optimal values for the scaling parameters $\lambda, \nu$:
   $$
   \lambda^{\star}:=\min \left(\frac{1-\eta}{(1-\eta)^{2}+\omega}, 1\right), \quad \nu^{\star}:=\min \left(\frac{1-\eta}{(1-\eta)^{2}+\omega_{\mathrm{av}}}, 1\right) .
   $$
   Given $\lambda \in(0,1]$ and $\nu \in(0,1]$, we define for convenience
   $$
   r:=(1-\lambda+\lambda \eta)^{2}+\lambda^{2} \omega, \quad r_{\mathrm{av}}:=(1-\nu+\nu \eta)^{2}+\nu^{2} \omega_{\mathrm{av}} .
   $$
   as well as $s^{\star}:=\sqrt{\frac{1+r}{2 r}}-1$ and $\theta^{\star}:=s^{\star}\left(1+s^{\star}\right) \frac{r}{r_{\text {av }}}$.

   \begin{theorem}[Linear Convergence under PL condition]\label{thm:main_pl}
      Suppose Assumption~\ref{assump:smooth} and PL assumption~\ref{assump:pl} are satisfied. Similarly, for every $t\geq 0$, 

      \begin{equation}
         \mathbb{E}\left[\Psi^{t}\right] \leq\left(\max \left(1-\gamma \mu, \frac{r+1}{2}\right)\right)^{t} \Psi^{0}.
         \end{equation}
   \end{theorem}


\section{Experiments}
For our experiments on logistic regression and least squares, we use the dataset \texttt{mushrooms}, \texttt{phishing}, \texttt{a9a} and \texttt{w8a} from LibSVM~\cite{chang2011libsvm}. We randomly devide all datapoints into $n$ different workers, where each worker has $m$ datapoints. In our practice, we choose $m \in \{20, n_{\mathrm{all}}\}$, where $\mathrm{all}$ is the total datapoints in a given dataset. 

\subsection{Logistic regression with nonconvex regularizer}
We first consider the logistic regression optimization problem with nonconvex regularizer, 

\begin{equation}
   f(x)=\frac{1}{n} \sum_{i=1}^{n} \log \left(1+\exp \left(-y_{i} a_{i}^{\top} x\right)\right)+\lambda \sum_{j=1}^{d} \frac{x_{j}^{2}}{1+x_{j}^{2}}, 
\end{equation}

where $a_{i} \in \mathbb{R}^{d}, y_{i} \in\{-1,1\}$ are the training data, and $\lambda>0$ is the regularizer parameter. We used $\lambda=0.1$ in all experiments.

\subsection{Least squares}
Here we consider the function that satisfies the PL condition. We use the standard least squares objective function, 

\begin{equation}
   f(x)=\frac{1}{n} \sum_{i=1}^{n}\left(a_{i}^{\top} x-b_{i}\right)^{2},
\end{equation}

where $a_{i} \in \mathbb{R}^{d}, y_{i} \in\{-1,1\}$ are the training data.

\subsection{Deep neural networks}
We also consider the deep neural networks experiments. To make fair comparison, we choose CIFAR-10 as our target dataset, which contains 50, 000 images for training and 10,000 images for testing. We randomly split the whole training set and testing set into n=5 equal part. Similar to \algname{EF21}~\cite{ric21}, we consider models including ResNet18 and VGG11. 
% \subsection{Algorithm Description}
% \begin{algorithm}[!htbp]
% \caption{EF-BV for single node}\label{alg:efbv_1}
% \begin{algorithmic}
% \Require stepsize $\gamma>0$, initial iterate $x^{0} \in \R^{d}$; initial shift {$h^0$} $\in \R^{d}$, \sybg{$\lambda\in (0, 1]$}, \sybr{$\nu \in (0,1]$} 
% \For{t = 0, 1, 2, $\cdots$}
%     \State Compute the gradient: $\nabla f\left(x^{t}\right)$
%     \State Compress shifted gradient: {$m^{t}$} $=\mathcal{C}(\nabla f\left(x^{t}\right)-$ {$h^{t})$}
%     \State Compute gradient estimator: $g^{t}=$ {$h^{t}$} + \sybr{$\nu$}{$m^{t}$}
%     \State Take proximal SGD step: $x^{t+1}=\operatorname{prox}_{\gamma R}\left(x^{t}-\gamma g^{t}\right)$
%     \State Update the shift: {$h^{t+1}$}={$h^{t}$} $+$ \sybg{$\lambda$} {$m^{t}$}
% \EndFor 
% \end{algorithmic}
% \end{algorithm}

\bibliography{mybib}
\bibliographystyle{iclr2021_conference}

\newpage
\appendix


\section{Appendix}
\subsection{Missing proofs}
\subsubsection{Missing proof of Theorem~\ref{thm:main}}
$g_i^t = h_i^t + \nu \C(\nabla f_i (x^t) - h_i^t); \quad h_i^{t+1} = h_i^t + \lambda \C(\nabla f_i (x^t) - h_i^t)$

\begin{proof}
   We first bound $\E\left[\|g_i^{t+1} - \nabla f_i(x^{t})\|^2\right]$. 

   Given any compressor $\mathcal{C}$, we can do a {\it bias-variance decomposition} of the compression error (see Eqn.~\ref{eqbiva}). 
   % That means, for every $x\in\mathbb{R}^d$,
   % \begin{equation}
   %  \mathbb{E}\big[\|\mathcal{C}(x)-x\|^2\big] = {\underbrace{\big\| \mathbb{E}[\mathcal{C}(x)]-x\big\|}_{\text{bias}}}^2 + \underbrace{\mathbb{E}\Big[\big\|\mathcal{C}(x)-\mathbb{E}[\mathcal{C}(x)]\big\|^2\Big]}_{\text{variance}}.\label{eqbiva}
   %  \end{equation}
   For every $t\geq 0$, define $W^t = \{x^t, h^t, (h_i^t)_{i=1}^n\}$,
   \begin{equation}\label{eq0001}
   \begin{aligned}
      \Exp{\sqnorm{g^{t+1}-\nabla f(x^t)} | W^t} &=\Exp{\sqnorm{\frac{1}{n}\sum_{i=1}^n \Big(h_i^{t}-\nabla f_i(x^t) +\nu \mathcal{C}_i^t\big(\nabla f_i(x^t)-h_i^t\big) \Big) } | W^t}\\
      &\stackrel{(\ref{eqbiva})}{=}\sqnorm{\frac{1}{n}\sum_{i=1}^n \Big(h_i^{t}-\nabla f_i(x^t) +\nu \Exp{\mathcal{C}_i^t\big(\nabla f_i(x^t)-h_i^t\big)} \Big) | W^t} \\
      &\quad+\nu^2\Exp{\sqnorm{\frac{1}{n}\sum_{i=1}^n \Big( \mathcal{C}_i^t\big(\nabla f_i(x^t)-h_i^t\big)-\Exp{ \mathcal{C}_i^t\big(\nabla f_i(x^t)-h_i^t\big) } \Big) } | W^t}\\
      &\leq \sqnorm{\frac{1}{n}\sum_{i=1}^n \Big(h_i^{t}-\nabla f_i(x^t) +\nu \Exp{\mathcal{C}_i^t\big(\nabla f_i(x^t)-h_i^t\big)} \Big) | W^t}\\
      &\quad+\nu^2 \frac{\oma}{n}\sum_{i=1}^n \sqnorm{\nabla f_i(x^t)-h_i^t },
   \end{aligned}      
   \end{equation}
   where the last inequality follows from Eqn.~7 in \cite{condat2022ef}. In addition, using Jensen's inequality and the definition of general compressor (Eqn.~\ref{def:compressors}), 
 
   % \kai{TODO: check where W^t can be omit from EF21-Lemma2}
   \begin{equation}\label{eq0002}
   \begin{aligned}
      &\left\|\frac{1}{n}\sum_{i=1}^n \Big(h_i^{t}-\nabla f_i(x^t) +\nu \Exp{\mathcal{C}_i^t\big(\nabla f_i(x^t)-h_i^t\big)} \Big) | W^t\right\|\\
      &\quad \leq \left\|\frac{1}{n}\sum_{i=1}^n \Big(\nu\big(h_i^{t}-\nabla f_i(x^t)\big) +\nu \Exp{\mathcal{C}_i^t\big(\nabla f_i(x^t)-h_i^t\big)} \Big) | W^t\right\| + (1-\nu)\left\|\frac{1}{n}\sum_{i=1}^n \big(h_i^{t}-\nabla f_i(x^t)\big)\right\|\\
      &\quad \leq \frac{\nu}{n}\sum_{i=1}^n\left\|h_i^{t}-\nabla f_i(x^t)+ \Exp{\mathcal{C}_i^t\big(\nabla f_i(x^t)-h_i^t\big)} | W^t\right\| + \frac{1-\nu}{n}\sum_{i=1}^n \left\|h_i^{t}-\nabla f_i(x^t)\right\|\\
      &\quad \leq  \frac{\nu\eta}{n}\sum_{i=1}^n \left\|\nabla f_i(x^t)-h_i^{t}\right\|+ \frac{1-\nu}{n}\sum_{i=1}^n \left\|\nabla f_i(x^t)-h_i^{t}\right\|\\
      & \quad = \frac{1-\nu+\nu\eta}{n}\sum_{i=1}^n \left\|\nabla f_i(x^t)-h_i^{t}\right\|.
   \end{aligned}
   \end{equation}
   Therefore, 
   \begin{align*}
   \sqnorm{\frac{1}{n}\sum_{i=1}^n \Big(h_i^{t}-\nabla f_i(x^t) +\nu \Exp{\mathcal{C}_i^t\big(\nabla f_i(x^t)-h_i^t\big)} \Big) | W^t} \leq \frac{(1-\nu+\nu\eta)^2}{n}\sum_{i=1}^n \sqnorm{\nabla f_i(x^t)-h_i^{t}}.
   \end{align*}

   Putting Eq.~\ref{eq0002} into Eq.~\ref{eq0001}, we have 
   
   \begin{align*}
   \Exp{\sqnorm{g^{t+1}-\nabla f(x^t)}| W^t} &\leq 
   \left((1-\nu+\nu\eta)^2+\nu^2\oma\right)\frac{1}{n}\sum_{i=1}^n \sqnorm{\nabla f_i(x^t)-h_i^{t}}.
   \end{align*}

   Using the Tower property and we obtain the unconditioned term,
   \begin{align*}
      \Exp{\sqnorm{g^{t+1}-\nabla f(x^t)}} &\leq 
      \left((1-\nu+\nu\eta)^2+\nu^2\oma\right)\frac{1}{n}\sum_{i=1}^n \Exp{\sqnorm{\nabla f_i(x^t)-h_i^{t}}}.
      \end{align*}
    
   Since we have the property \citep[Lemma 4]{ric21}, for every $t\geq 0$,
   \begin{align}
   f(x^{t+1}) -f^{\inf} &\leq f(x^t)  -f^{\inf} -\frac{\gamma}{2} \sqnorm{\nabla f(x^t)} +\frac{ \gamma }{2}\sqnorm{g^{t+1}-\nabla f(x^t)} + \left(\frac{ L}{2}-\frac{1}{2\gamma}\right)\sqnorm{x^{t+1}-x^t}.\notag
   \end{align}

   Thus, for every $t\geq 0$, conditionally on $x^t$, $h^t$ and $(h_i^t)_{i=1}^n$,
    \begin{align*}
   \Exp{f(x^{t+1}) -f^{\inf}} &\leq \Exp{f(x^t)  -f^{\inf}} -\frac{\gamma}{2} \Exp{\sqnorm{\nabla f(x^t)}}\notag\\
   &\quad+ \left(\frac{ L}{2}-\frac{1}{2\gamma}\right)\Exp{\sqnorm{x^{t+1}-x^t}}  + \frac{ \gamma }{2}\Exp{\sqnorm{g^{t+1}-\nabla f(x^t)}} .\notag
   \end{align*}
   
   Now, let us study the control variates $h_i^t$. Let $s>0$. Using the Peter--Paul inequality $\|a+b\|^2 \leq (1+s) \|a\|^2 + (1+s^{-1}) \|b\|^2$, for any vectors $a$ and $b$, 
   we have, for every $t\geq 0$ and $i\in\mathcal{I}_n$,
   \begin{align*}
   \sqnorm{\nabla f_i(x^{t+1})-h_i^{t+1}}&=\sqnorm{h_i^{t}-\nabla f_i(x^{t+1}) +\lambda \mathcal{C}_i^t\big(\nabla f_i(x^t)-h_i^t\big)  }\\
   &\leq (1+s)\sqnorm{h_i^{t}-\nabla f_i(x^{t}) +\lambda \mathcal{C}_i^t\big(\nabla f_i(x^t)-h_i^t\big)  }\\ 
   &\quad+(1+s^{-1})\sqnorm{\nabla f_i(x^{t+1})-\nabla f_i(x^{t})}\\
   &\leq (1+s)\sqnorm{h_i^{t}-\nabla f_i(x^{t}) +\lambda \mathcal{C}_i^t\big(\nabla f_i(x^t)-h_i^t\big)  } \\
   &\quad+(1+s^{-1})L_i^2\sqnorm{x^{t+1}-x^{t}}.
   \end{align*}
   Moreover,  conditionally on $x^t$, $h^t$ and $(h_i^t)_{i=1}^n$, 
   \begin{align*}
   \Exp{\sqnorm{h_i^{t}-\nabla f_i(x^{t}) +\lambda \mathcal{C}_i^t\big(\nabla f_i(x^t)-h_i^t\big)  }}&=\sqnorm{h_i^{t}-\nabla f_i(x^{t}) +\lambda \Exp{\mathcal{C}_i^t\big(\nabla f_i(x^t)-h_i^t\big)} }\\
   &\quad+\lambda^2\Exp{\sqnorm{ \mathcal{C}_i^t\big(\nabla f_i(x^t)-h_i^t\big)-\Exp{ \mathcal{C}_i^t\big(\nabla f_i(x^t)-h_i^t\big) }  }}\\
   &\leq\sqnorm{h_i^{t}-\nabla f_i(x^{t}) +\lambda \Exp{\mathcal{C}_i^t\big(\nabla f_i(x^t)-h_i^t\big)} }\\
   &\quad+\lambda^2\omega \sqnorm{\nabla f_i(x^t)-h_i^t}.
   \end{align*}
   In addition,
   \begin{align*}
   \left\|h_i^{t}-\nabla f_i(x^{t}) +\lambda \Exp{\mathcal{C}_i^t\big(\nabla f_i(x^t)-h_i^t\big)}\right\|& \leq \left\|\lambda\big(h_i^{t}-\nabla f_i(x^{t})\big) +\lambda \Exp{\mathcal{C}_i^t\big(\nabla f_i(x^t)-h_i^t\big)}\right\|\notag\\
   &\quad + (1-\lambda)\left\|h_i^{t}-\nabla f_i(x^t)\right\|\\
   &\leq  \lambda\eta\left\|\nabla f_i(x^t)-h_i^{t}\right\|+ (1-\lambda)\left\|\nabla f_i(x^t)-h_i^{t}\right\|\notag\\
   &=(1-\lambda+\lambda\eta)\left\|\nabla f_i(x^t)-h_i^t\right\|.
   \end{align*}
   Therefore, conditionally on $x^t$, $h^t$ and $(h_i^t)_{i=1}^n$, 
   \begin{align*}
   \Exp{\sqnorm{h_i^{t}-\nabla f_i(x^{t}) +\lambda \mathcal{C}_i^t\big(\nabla f_i(x^t)-h_i^t\big)  }}\leq\big((1-\lambda+\lambda\eta)^2+\lambda^2\omega\big) \sqnorm{\nabla f_i(x^t)-h_i^t}
   \end{align*}
   and
   \begin{align*}
   \Exp{\sqnorm{\nabla f_i(x^{t+1})-h_i^{t+1}}}&\leq (1+s)\big((1-\lambda+\lambda\eta)^2+\lambda^2\omega\big) \sqnorm{\nabla f_i(x^{t})-h_i^{t}}\\
   &\quad+(1+s^{-1})L_i^2\Exp
   {\sqnorm{x^{t+1}-x^{t}}},
   \end{align*}
   so that
   \begin{align*}
   \Exp{\frac{1}{n}\sum_{i=1}^n \sqnorm{\nabla f_i(x^{t+1})-h_i^{t+1}}}&
   \leq (1+s)\big((1-\lambda+\lambda\eta)^2+\lambda^2\omega\big) \frac{1}{n}\sum_{i=1}^n \sqnorm{\nabla f_i(x^t)-h_i^{t}}\\
   &\quad+(1+s^{-1})\tilde{L}^2\Exp
   {\sqnorm{x^{t+1}-x^{t}}}.
   \end{align*}

   Let $\theta>0$; its value will be set to $\theta^\star$ later on. We introduce the Lyapunov function, for every $t\geq 0$,
   \begin{equation*}
   \Psi^t \eqdef f(x^t)-f^{\inf} + \frac{\gamma}{2\theta}  \frac{1}{n}\sum_{i=1}^n \sqnorm{\nabla f_i(x^t)-h_i^{t}}.
   \end{equation*}
   Hence, for every $t\geq 0$, conditionally on $x^t$, $h^t$ and $(h_i^t)_{i=1}^n$, we have
   \begin{align}
   \Exp{\Psi^{t+1}} &\leq \Exp{f(x^t) - f^{\inf}} - \frac{\gamma}{2} \Exp{\sqnorm{\nabla f(x^t)}}\notag \\
   &\quad +\frac{ \gamma }{2\theta}\Big( \theta\big((1-\nu+\nu\eta)^2+\nu^2\oma\big)\notag+(1+s)\big((1-\lambda+\lambda\eta)^2+\lambda^2\omega\big)
   \Big) \frac{1}{n}\sum_{i=1}^n \sqnorm{\nabla f_i(x^t)-h_i^{t}}\label{eqq1}\\
   &\quad+ \left(\frac{ L}{2}-\frac{1}{2\gamma}+\frac{\gamma}{2\theta}(1+s^{-1})\tilde{L}^2\right)\!\Exp{\sqnorm{x^{t+1}-x^t}}.\notag
   \end{align}
   Making use of $r$ and $r_{\mathrm{av}}$ and setting $\theta = s(1+s)\frac{r}{r_{\mathrm{av}}}$, 
   we can rewrite the above equation as:
   \begin{align*}
   \Exp{\Psi^{t+1}} &\leq \Exp{f(x^t) - f^{\inf}} - \frac{\gamma}{2} \Exp{\sqnorm{\nabla f(x^t)}} +\frac{ \gamma }{2\theta}\Big( \theta r_{\mathrm{av}}
   +(1+s)r
   \Big) \frac{1}{n}\sum_{i=1}^n \sqnorm{\nabla f_i(x^t)-h_i^{t}}\\
   &\quad+ \left(\frac{ L}{2}-\frac{1}{2\gamma}+\frac{\gamma}{2\theta}(1+s^{-1})\tilde{L}^2\right)\!\Exp{\sqnorm{x^{t+1}-x^t}}\\
   &=\Exp{f(x^t) - f^{\inf}} - \frac{\gamma}{2} \Exp{\sqnorm{\nabla f(x^t)}} +\frac{ \gamma }{2\theta} (1+s)^2 
   \frac{r}{n}\sum_{i=1}^n \sqnorm{\nabla f_i(x^t)-h_i^{t}}\\
   &\quad+ \left(\frac{ L}{2}-\frac{1}{2\gamma}+\frac{\gamma}{2s^2}\frac{r_{\mathrm{av}}}{r}\tilde{L}^2\right)\!\Exp{\sqnorm{x^{t+1}-x^t}}.
   \end{align*}
   We now choose $\gamma$ small enough so that 
   \begin{equation}
   L-\frac{1}{\gamma}+\frac{\gamma}{s^2}\frac{r_{\mathrm{av}}}{r}\tilde{L}^2 \leq 0.\label{eqgreg}
   \end{equation}
   A sufficient condition for \eqref{eqgreg} to hold is \citep[Lemma 5]{ric21}:
   \begin{equation}
   0<\gamma \leq \frac{1}{L+\tilde{L}\sqrt{\frac{r_{\mathrm{av}}}{r}}\frac{1}{s}}.\label{eqgamfek}
   \end{equation}
   Then, assuming that \eqref{eqgamfek} holds, we have, for every $t\geq 0$, conditionally on $x^t$, $h^t$ and $(h_i^t)_{i=1}^n$,
   \begin{align*}
   \Exp{\Psi^{t+1}} &\leq \Exp{f(x^t) - f^{\inf}} - \frac{\gamma}{2} \Exp{\sqnorm{\nabla f(x^t)}} +\frac{ \gamma }{2\theta} (1+s)^2 
   \frac{r}{n}\sum_{i=1}^n \sqnorm{\nabla f_i(x^t)-h_i^{t}}.\\
   \end{align*}

   Using the Tower property, we have,
   \begin{align*}
      \Exp{\Psi^{t+1}} \leq \Exp{\Psi^t} - \frac{\gamma}{2}\Exp{\sqnorm{\nabla f(x^t)}}.
   \end{align*}

   By summing up inequalities for $t=0, \cdots, T-1$, we get 
   \begin{align*}
      0 \leq \Exp{\Psi^(T)} \leq \Exp{\Psi^0} - \frac{\gamma}{2} \sum_{t=0}^{T-1} \Exp{\sqnorm{\nabla f(x^t)}}.
   \end{align*}

   % We define $\detal^t \eqdef \Exp{f(x^t) - f^{\inf}}, s^t \eqdef \Exp{}$  
   Multiplying both sides by $\frac{2}{\gamma T}$, after rearranging we get
   \begin{align*}
      \sum_{t=0}^{T-1} \frac{1}{T}\Exp{\sqnorm{\nabla f(x^t)}} \leq \frac{2}{\gamma T} \Psi(0),
   \end{align*}

   % We see that $s$ must be small enough so that $(1+s)^2 r <1$; this is the case with 
   % $s=s^\star$, so that $(1+s^\star)^2 r = \frac{r+1}{2}<1$. 
   % Therefore, we set $s=s^\star$, and, accordingly, $\theta=\theta^\star$. Then, for every $t\geq 0$,
   % conditionally on $x^t$, $h^t$ and $(h_i^t)_{i=1}^n$,
   % \begin{align*}
   % \Exp{\Psi^{t+1}} 
   % &\leq \max\big(1-\gamma\mu, {\frac{r+1}{2}}\big) \Psi^t.
   % \end{align*}
   where the left hand side can be interpreted as $\mathrm{E}\left[\left\|\nabla f\left(\hat{x}^{T}\right)\right\|^{2}\right]$, where $\hat{x}^{T}$ is chosen from $x^{0}, x^{1}, \ldots, x^{T-1}$ uniformly at random.

   \end{proof}

   \subsubsection{Missing proof of Theorem~\ref{thm:main_pl}}
   This proof is under \algname{EF-BF}~\cite{condat2022ef} Theorem.1. 

   \section{EF-BV Experiments Revisit}
   In thi section, we revisit the experiments in \algname{EF-BV} on top of the refactored code. We fixed a few minor bugs.

   For all experiments, we consider the logistic regression with convex regularizer, 

   \begin{equation}
      f(x)=\frac{1}{n} \sum_{i=1}^{n} \log \left(1+\exp \left(-y_{i} a_{i}^{\top} x\right)\right)+ \frac{\lambda}{2}\sqnorm{x},
   \end{equation}
   
   where $a_{i} \in \mathbb{R}^{d}, y_{i} \in\{-1,1\}$ are the training data, and $\lambda>0$ is the regularizer parameter. We used $\lambda=0.1$ in all experiments.

   For baselines, since \algname{EF21}~\cite{ric21} outperforms all other methods, we only compare our method with \algname{EF21}. 

   % \subsection{Ablations with CompK compressor}

   % \subsection{Ablations with MixK compressor}
\end{document}
