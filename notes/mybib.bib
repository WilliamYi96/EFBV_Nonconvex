@article{mishchenko2022proxskip,
  title={ProxSkip: Yes! Local Gradient Steps Provably Lead to Communication Acceleration! Finally!},
  author={Mishchenko, Konstantin and Malinovsky, Grigory and Stich, Sebastian and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2202.09357},
  year={2022}
}

@article{fatkhullin2021ef21,
  title={EF21 with bells \& whistles: Practical algorithmic extensions of modern error feedback},
  author={Fatkhullin, Ilyas and Sokolov, Igor and Gorbunov, Eduard and Li, Zhize and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2110.03294},
  year={2021}
}

@inproceedings{kovalev2020don,
  title={Don’t jump through hoops and remove those loops: SVRG and Katyusha are better without the outer loop},
  author={Kovalev, Dmitry and Horv{\'a}th, Samuel and Richt{\'a}rik, Peter},
  booktitle={Algorithmic Learning Theory},
  pages={451--467},
  year={2020},
  organization={PMLR}
}

@article{qian2021svrg,
  title={L-SVRG and L-Katyusha with Arbitrary Sampling},
  author={Qian, Xun and Qu, Zheng and Richt{\'a}rik, Peter},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={112},
  pages={1--47},
  year={2021}
}

@article{chang2011libsvm,
  title={LIBSVM: a library for support vector machines},
  author={Chang, Chih-Chung and Lin, Chih-Jen},
  journal={ACM transactions on intelligent systems and technology (TIST)},
  volume={2},
  number={3},
  pages={1--27},
  year={2011},
  publisher={Acm New York, NY, USA}
}

@article{condat2022ef,
  title={EF-BV: A Unified Theory of Error Feedback and Variance Reduction Mechanisms for Biased and Unbiased Compression in Distributed Optimization},
  author={Condat, Laurent and Yi, Kai and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2205.04180},
  year={2022}
}


@unpublished{Cnat,
  author        = {Horv\'{a}th, Samuel and Ho, Chen-Yu and Horv\'{a}th, Ludov\'{i}t and Sahu, Atal Narayan and Canini, Marco and Richt\'{a}rik, Peter},
  title         = {Natural Compression for Distributed Deep Learning},
  note       = {preprint arXiv:1905.10988},
  year          = {2019},
}

@InProceedings{kar16,
author="Karimi, Hamed
and Nutini, Julie
and Schmidt, Mark",
editor="Frasconi, Paolo
and Landwehr, Niels
and Manco, Giuseppe
and Vreeken, Jilles",
title="Linear Convergence of Gradient and Proximal-Gradient Methods Under the {P}olyak-{\L}ojasiewicz Condition",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="795--811",
}

@unpublished{sze22,
author = "R. Szlendak and A. Tyurin and P. Richt{\'a}rik",
title = "Permutation Compressors for Provably Faster Distributed Nonconvex Optimization", 
year = "2021",
note = "preprint arXiv:2110.03300, accepted at ICLR 2022"
}



@article{att09,
author = "H. Attouch and J. Bolte",
title = "On the convergence of the proximal algorithm for nonsmooth functions involving analytic features", 
journal = "Math. Program.",
year = "2009",
volume = "116",
pages = "5--116"
}

@article{con22,
author = "L. Condat and G. Malinovsky and P. Richt{\'a}rik",
title = "Distributed Proximal Splitting Algorithms with Rates and Acceleration", 
journal = "Frontiers in Signal Processing",
year = "2022",
volume = "1",
month = jan
}


@conference{mis20,
	author = "Konstantin Mishchenko and Filip Hanzely and Peter Richt{\'a}rik",
	title = "99\% of worker-master communication in distributed optimization is not needed",
	booktitle = "Proc. of 36th Conf. on Uncertainty in Artificial Intelligence (UAI)",
	volume ="124", 
	pages ="979--988",
	year = "2020",
}


@article{saf21,
author = "Mher Safaryan and Egor Shulgin and Peter Richt{\'a}rik",
title = "Uncertainty principle for communication compression in distributed and federated learning and the search for an optimal compressor", 
journal = "Information and Inference: A Journal of the IMA",
year = "2021",
}

@unpublished{con21,
  author  = "L. Condat and P. {Richt\'{a}rik}",
  title   = "{MURANA: A} Generic Framework for Stochastic Variance-Reduced Optimization",
 note = {preprint arXiv:2106.03056v1},
  year    = {2021},
}


@article{ver21,
author = "J. Verbraeken and M. Wolting and J. Katzy and J. Kloppenburg and T. Verbelen and J. S. Rellermeyer",
title = "A survey on distributed machine learning", 
journal = "ACM Computing Surveys",
year = "2021",
month = mar,
number = "2",
volume = "53",
pages = "1--33"
}

@article{kai19,
author = "P. Kairouz and others",
title = "Advances and Open Problems in Federated Learning", 
journal = "Foundations and Trends in Machine Learning",
year = "2021",
number = "1--2",
volume = "14"
}

@article{han19,
  title={One Method to Rule Them All: Variance Reduction for Data, Parameters and Many New Methods},
  author={Hanzely, Filip and Richt{\'a}rik, Peter},
  journal={preprint arXiv:1905.11266},
  year={2019}
}


@unpublished{xu20,
	author = "H. Xu and C.-Y. Ho and A. M. Abdelmoniem and A. Dutta and E. H. Bergou and K. Karatsenidis and M. Canini and P. Kalnis",
	title = "Compressed communication for distributed deep learning: {S}urvey and quantitative evaluation",
	year = "2020",
	note = "Technical report, KAUST"
}

@conference{xu21,
	author = "H. Xu and C.-Y. Ho and A. M. Abdelmoniem and A. Dutta and E. H. Bergou and K. Karatsenidis and M. Canini and P. Kalnis",
	title = "{GRACE: A compressed communication framework for distributed machine learning}",
	booktitle = "Proc. of 41st IEEE Int. Conf. Distributed Computing Systems (ICDCS)",
	year = "2021",
}


@conference{mcm17,
  author    = {McMahan, H Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and Ag\"{u}era y Arcas, Blaise},
  title     = {Communication-efficient learning of deep networks from decentralized data},
  booktitle = {Proc. of Int. Conf. Artificial Intelligence and Statistics (AISTATS)},
  year      = {2017},
  volume = "PMLR 54",
}


@conference{def14,
  title="{SAGA: A} fast incremental gradient method with support for non-strongly convex composite objectives",
  author="A. Defazio and F. Bach and S. {Lacoste-Julien}",
  booktitle="Proc. of 28th Conf. Neural Information Processing Systems (NIPS)",
  pages={1646--1654},
  year={2014}
}

@conference{sei14,
title = "1-Bit Stochastic Gradient Descent and Application to Data-Parallel Distributed Training of Speech {DNNs}",
author = {Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
booktitle = {Proc. of Annual Conf. of  Int. Speech Communication Association (Interspeech)},
year = {2014},
}

@conference{ric21,
title = "{EF21}: {A} New, Simpler, Theoretically Better, and Practically Faster Error Feedback",
author = "Peter Richt{\'a}rik and Igor Sokolov and Ilyas Fatkhullin",
booktitle = "Proc. of 35th Conf. Neural Information Processing Systems (NeurIPS)",
year = {2021},
}

@unpublished{fat21,
  title="{EF21} with Bells {\&} Whistles: Practical Algorithmic Extensions of Modern Error Feedback",
  author="I. Fatkhullin and I. Sokolov and E. Gorbunov and Z. Li and P. Richt{\'a}rik",
  note = "preprint arXiv:2110.03294",
  year={2021}
}

@conference{ja2016,
title	= {Federated Learning: Strategies for Improving Communication Efficiency},
author	= {Jakub Kone\v{c}n\'{y} and H. Brendan McMahan and Felix X. Yu and Peter Richt\'{a}rik and Ananda Theertha Suresh and Dave Bacon},
year	= {2016},
booktitle	= {NIPS Workshop on Private Multi-Party Machine Learning}
}


@unpublished{qia19,
  title="{MISO} is Making a Comeback With Better Proofs and Rates",
  author="X. Qian and A. Sailanbayev and K. Mishchenko and P. Richt{\'a}rik",
  year={2019},
  month = jun,
  note = "arXiv:1906.01474"
}

@conference{gaz19,
  title="Optimal Mini-Batch and Step Sizes for {SAGA}",
  author="N. Gazagnadou and R. Gower and J. Salmon",
  booktitle="Proc. of 36th Int. Conf. Machine Learning (ICML)",
  pages="2142--2150",
  volume = "PMLR 97",
  year="2019"
}

@conference{gow19,
  author    = "R. M. Gower and N. Loizou and X. Qian and A. Sailanbayev and E. Shulgin and P. Richt\'{a}rik",
  title     = "{SGD}: {G}eneral Analysis and Improved Rates",
  booktitle="Proc. of 36th Int. Conf. Machine Learning (ICML)",  year      = {2019},
  volume = "PMLR 97",
  pages     = {5200--5209}
}

@conference{seb19,
  author    = "O. Sebbouh and N. Gazagnadou and S. Jelassi and F. Bach and R. Gower",
  title     = "Towards closing the gap between the theory and practice of {SVRG}",
  year={2019},
   booktitle = "Proc. of 33rd Conf. Neural Information Processing Systems (NeurIPS)",
}
 

@unpublished{bac21,
  title="Learning Theory from First Principles",
  author="F. Bach",
  note = "Draft of a book, version of Sept. 6, 2021",
  year={2021}
}


@conference{liu20,
  title="A double residual compression algorithm for efficient distributed learning",
  author="X. Liu and Y. Li and J. Tang and M. Yan",
  booktitle="Proc. of 23rd Int. Conf. Artificial Intelligence and Statistics (AISTATS)",
  pages="133--143",
  volume = "PMLR 108",
  year={2020}
}

@unpublished{phi20,
  title="Bidirectional compression in heterogeneous settings for distributed or federated learning with partial participation: tight convergence guarantees",
  author="C. Philippenko and A. Dieuleveut",
    year={2020},
    note = "arXiv:2006.14591"
}

@conference{tan19,
  title="DoubleSqueeze: {P}arallel stochastic gradient descent with double-pass error-compensated compression",
  author="H. Tang and C. Yu and X. Lian and T. Zhang and J. Liu",
  booktitle="Proc. of Int. Conf. Machine Learning (ICML)",
  pages="6155--6165",
  year={2019}
}


@conference{kov20,
  title="Don't Jump Through Hoops and Remove Those Loops: {SVRG and Katyusha} are Better Without the Outer Loop",
  author="D. Kovalev and S. {Horv\'ath} and P. {Richt\'arik}",
  booktitle="Proc. of 31st Int. Conf. Algorithmic Learning Theory (ALT)",
  volume = "PMLR 117",
  pages="451--467",
  year={2020}
}

@article{qia21,
  title="{L-SVRG and L-Katyusha} with Arbitrary Sampling",
  author="X. Qian and Z. Qu and P. {Richt\'arik}",
   year={2021},
   pages = {1--47},
   volume = {22},
   number = {112},
   journal = {Journal of Machine Learning Research}
}

@conference{li21,
  title="{CANITA: Faster} Rates for Distributed Convex Optimization with Communication Compression",
  author="Z. Li and P. {Richt\'arik}",
   year={2021},
   booktitle = "Proc. of 35th Conf. Neural Information Processing Systems (NeurIPS)"
}

@conference{li2020,
  title="Acceleration for compressed gradient descent in distributed and federated optimization",
  author="Z. Li and D. Kovalev and X. Qian and P. {Richt\'arik}",
   year={2020},
  booktitle = "Proc. of 37th Int. Conf. Machine Learning (ICML)"
  }


@conference{gor20,
  title="Linearly Converging Error Compensated {SGD}",
  author="E. Gorbunov and D. Kovalev and D. Makarenko and P. {Richt\'arik}",
   year={2020},
   booktitle = "Proc. of 34th Conf. Neural Information Processing Systems (NeurIPS)",
}

@conference{zha13,
  author    = "L. Zhang and M. Mahdavi and R. Jin",
  title     = "Linear convergence with condition number independent access of full gradients",
    booktitle="Proc. of 27th Conf. Neural Information Processing Systems (NIPS)",
  year      = {2013},
}

@conference{joh13,
  author    = "R. Johnson and T. Zhang",
  title     = {Accelerating stochastic gradient descent using predictive variance reduction},
    booktitle="Proc. of 27th Conf. Neural Information Processing Systems (NIPS)",
  year      = {2013},
  pages     = {315--323},
}

@article{xia14,
  author  = {L. Xiao and T. Zhang},
  title   = {A proximal stochastic gradient method with progressive variance reduction},
  journal = "SIAM J.  Optim.",
  year    = {2014},
  volume  = {24},
  number  = {4},
  pages   = {2057--2075},
}


@Article{L-SVRG-AS,
  author  = {Qian, Xun and Qu, Zheng and Richt\'{a}rik, Peter},
  title   = {{L-SVRG} and {L-K}atyusha with arbitrary sampling},
  journal = {preprint arXiv:1906.01481},
  year    = {2019},
}


@conference{hof15,
	author = "T. Hofmann and
A. Lucchi and S. {Lacoste-Julien} and
B. {McWilliams}",
	title = "Variance Reduced Stochastic Gradient Descent with Neighbors",
	booktitle = "Proc. of 29th Conf. Neural Information Processing Systems (NIPS)",
	year = "2015",
	pages="1509--1519"
}

@conference{ali17,
	author = "Alistarh, D. and Grubic, D. and Li, J. and Tomioka, R. and Vojnovic, M.",
	title = "{QSGD: Communication-efficient SGD via gradient quantization and encoding}",
	booktitle = "Proc. of 31st Conf. Neural Information Processing Systems (NIPS)",
	year = "2017",
	pages="1709--1720"
}

@conference{ali18,
  author    = {Dan Alistarh and Torsten Hoefler and Mikael Johansson and Sarit Khirirat and Nikola Konstantinov and C. Renggli},
  title     = {The Convergence of Sparsified Gradient Methods},
  booktitle = "Proc. of Conf. Neural Information Processing Systems (NeurIPS)",
  year      = {2018},
location = "Montreal, Canada"
}

@unpublished{bez20,
  author    = "A. Beznosikov and S. Horv{\'a}th and P. Richt{\'a}rik and M. Safaryan",
  title     = "On biased compression for distributed learning",
note = "preprint arXiv:2002.12410",
  year      = {2020},
}


@article{sat20,
  author="F. Sattler and S. Wiedemann and {K.-R. M\"uller} and W. Samek",
  journal="IEEE Trans. Neural Networks and Learning Systems", 
  title="Robust and Communication-Efficient Federated Learning From Non-i.i.d. Data", 
  year={2020},
  volume = "31",
  number = "9",
  pages="3400--3413"}

@conference{wen17,
	author = "Wen, W. and Xu, C. and Yan, F. and Wu, C. and Wang, Y. and Chen, Y. and Li,
H.",
	title = "{TernGrad: Ternary} gradients to reduce communication in distributed deep learning",
	booktitle = "Proc. of 31st Conf. Neural Information Processing Systems (NIPS)",
	year = "2017",
	pages="1509--1519"
}

@unpublished{mis19,
title = "Distributed Learning with Compressed Gradient Differences",
author = "K. Mishchenko and E. Gorbunov and M. {Tak\'a\v{c}} and P. {Richt\'arik}",
note = "arXiv:1901.09269",
year = "2019"
}

@unpublished{hor19,
title = "Stochastic Distributed Learning with Gradient Quantization and Variance Reduction",
author = "S. {Horv\'ath} and D. Kovalev and K. Mishchenko and S. Stich and P. {Richt\'arik}",
note = "arXiv:1904.05115",
year = "2019"
}

@article{gow20a,
  author="R. M. Gower and M. Schmidt and F. Bach and P. {Richt\'arik}",
  journal="Proc. of the IEEE", 
  title="Variance-Reduced Methods for Machine Learning", 
  year={2020},
  month = nov,
  volume = "108",
  number = "11",
  pages="1968--1983"}
  

@article{ric16,
  author="P. {Richt\'arik} and M. {Tak\'a\v{c}}",
  journal="Math.  Program.", 
  title="Parallel coordinate descent methods for big data optimization", 
  year={2016},
  volume = "156",
  pages="433--484"}
  
  @article{gow20,
  author="R. M. Gower and P. {Richt\'arik} and F. Bach",
  journal="Math. Program.", 
  title="Stochastic quasi-gradient methods: {V}ariance reduction via {J}acobian sketching", 
  year={2021},
  month = jul,
  volume = "188",
  pages = "135--192"
 }
  
   
@conference{dut20,
	Author = "A. Dutta and E. H. Bergou and A. M. Abdelmoniem and C. Y. Ho and A. N. Sahu and M. Canini and P. Kalnis",
	title = "On the discrepancy between the theoretical analysis and practical implementations of compressed communication for distributed deep learning",
	booktitle = "Proc. of AAAI Conf. Artificial Intelligence",
	year = {2020},
	pages = "3817--3824"
}
%number = "4",
%volume = "34",

@unpublished{alb20,
author = "A. Albasyoni and M. Safaryan and L. Condat and P. {Richt\'arik}",
title = "Optimal Gradient Compression for Distributed and Federated Learning", 
note = "preprint arXiv:2010.03246",
year = "2020"
}

@conference{wan18,
	Author = "Wangni, J. and Wang, J. and Liu, J. and Zhang, T. ",
	title = "Gradient sparsification for communication-efficient distributed optimization",
	booktitle = "Proc. of 32nd Conf. Neural Information Processing Systems (NeurIPS)",
	year = {2018},
	pages = "1306--1316"
}







@article{bas20,
  author={Basu, D. and Data, D. and Karakus, C. and Diggavi, S. N.},
  journal={IEEE Journal on Selected Areas in Information Theory}, 
  title="{Qsparse-Local-SGD: Distributed SGD With Quantization, Sparsification, and Local Computations}", 
  year={2020},
  volume={1},
  number={1},
  pages={217--226}}
  

@article{dos15,
	author = "A. Chambolle and C. Dossal",
	title = "On the Convergence of the Iterates of the {"Fast Iterative Shrinkage/Thresholding Algorithm"}",
	journal = "J. Optim. Theory Appl.",
	year = "2015",
	volume="166",
	pages="968--982"
}


@ARTICLE{com21,
  author={P. L. Combettes and J. C. Pesquet},
  journal={IEEE Transactions on Signal Processing}, 
  title={Fixed Point Strategies in Data Science}, 
  year={2021},
  note = "To appear"
  }

@conference{mal20,
	author = "G. Malinovsky and D. Kovalev and E. Gasanov and L. Condat and P. Richt{\'a}rik",
	title = "From local {SGD} to local fixed point methods for federated learning",
	booktitle = "Proc. of 37th Int. Conf. Machine Learning (ICML)",
	year = "2020"
}

@article{li20,
	author = "T. Li and A. K. Sahu and A. Talwalkar and V. Smith",
	title = "Federated Learning: Challenges, Methods, and Future Directions",
	journal = "IEEE Signal Processing Magazine",
	year = "2020",
	number="37",
	volume="3",
	pages="50--60"
}

@article{par14,
	author = "N. Parikh and S. Boyd",
	title = "Proximal Algorithms",
	journal = "Foundations and Trends in Optimization",
	year = "2014",
	number="1",
	volume="3",
	pages="127--239"
}

@article{bub14,
	author = "S. Bubeck",
	title = "Convex optimization: {A}lgorithms and complexity",
	journal = "Foundations and Trends in Machine Learning",
	year = "2014",
	volume="8",
	pages="231--357"
}

@inproceedings{dav16,
	author="D. Davis and W. Yin",
	editor="R. Glowinski and S. J. Osher and W. Yin",
	title="Convergence rate analysis of several splitting schemes",
	booktitle="Splitting Methods in Communication, Imaging, Science, and Engineering",
	year="2016",
	publisher="Springer International Publishing",
	address="Cham",
	pages="115--163",
}

@conference{com21a,
	author="P. L. Combettes and Z. C. Woodstock",
	title="A Fixed Point Framework for Recovering Signals from Nonlinear Transformations",
	year="2021",
	booktitle = "Proc. of 28th European Signal Processing Conference (EUSIPCO)",
	pages = "2120--2124"
}


@article{ric14,
author="P. Richt{\'a}rik and M. Tak{\'a}{\v{c}}",
title="Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function",
journal="Math. Program.",
year="2014",
month=apr,
volume="144",
pages="1--38",
}
%

@article{les16,
	author="L. Lessard and B. Recht and A. Packards",
	title="Analysis and Design of Optimization Algorithms via Integral Quadratic Constraints",
	journal="SIAM J. Optim.",
	year="2016",
	pages="57--95",
	volume = "26",
	number = "1"
}

@article{pes15,
	author = "J.-C. Pesquet and A. Repetti",
	title = "A Class of Randomized Primal-Dual Algorithms for Distributed Optimization",
	journal = "J. Nonlinear Convex Anal.",
	year = "2015",
	number="16",
	volume="12",
	month=dec,
}

@article{comy15,
	author = "P. L. Combettes and I. Yamada",
	title = "Compositions and convex combinations of averaged nonexpansive operators",
	journal = "Journal of Mathematical Analysis and Applications",
	year = "2015",
	number="1",
	volume="425",
	pages = "55--70"
}

@conference{yu13,
	author = "Y.-L. Yu",
	title = "On decomposing the proximal map",
	booktitle = "Proc. of 26th Int. Conf. Neural Information Processing Systems (NIPS)",
	year = "2013",
	pages = "91--99"
}


@Article{FEDOPT,
  author        = {Kone\v{c}n\'{y}, Jakub and McMahan, H. Brendan and Ramage, Daniel and Richt\'{a}rik, Peter},
  title         = {Federated optimization: distributed machine learning for on-device intelligence},
  journal       = {arXiv:1610.02527},
  year          = {2016},
}


@InProceedings{localGD,
  author      = {Khaled, Ahmed and Mishchenko, Konstantin and Richt\'{a}rik, Peter},
  title       = {First Analysis of Local {GD} on Heterogeneous Data},
  booktitle   = {NeurIPS Workshop on Federated Learning for Data Privacy and Confidentiality},
  year        = {2019},
 }

@InProceedings{GDCI,
  author      = {Khaled, Ahmed and Richt\'{a}rik, Peter},
  title       = {Gradient Descent with Compressed Iterates},
  booktitle   = {NeurIPS Workshop on Federated Learning for Data Privacy and Confidentiality},
  year        = {2019},
}

@Article{FL_survey_2019,
  author  = {Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia},
  title   = {Federated learning: challenges, methods, and future directions},
  journal = {preprint arXiv:1908.07873},
  year    = {2019},
}


@Article{FedAvg2016,
  author  = {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Ag\"{u}era y Arcas, Blaise},
  title   = {Federated Learning of Deep Networks using Model Averaging},
  journal = {preprint arXiv:1602.05629},
  year    = {2016},
}

@Article{Leaf2018,
  author  = {Caldas, Sebastian and Wua, Peter and Lia, Tian and Kone\v{c}n\'{y} and McMahan, Brendan and Smith, Virginia and Talwalkar, Ameet},
  title   = {Leaf: a benchmark for federated settings},
  journal = {preprint arXiv:1812.01097},
  year    = {2018},
}

@Article{FedMetaRecom2018,
  author  = {Chen, Fei and Dong, Zhenhua and Li, Zhenguo and He, Xiuqiang},
  title   = {Federated meta-learning for recommendation},
  journal = {preprint arXiv:1802.07876},
  year    = {2018},
}


@InProceedings{FL-MAML-Jakub,
  author    = {Jiang, Yihan and Kone\v{c}n\'{y}, Jakub and Rush, Keith and Kannan, Sreeram},
  title     = {Improving Federated Learning Personalization via Model Agnostic Meta Learning},
  booktitle = {NeurIPS Workshop on Federated Learning for Data Privacy and Confidentiality},
  year      = {2019},
}

@Article{FLvsSL2019,
  author  = {Abhishek Singh, Praneeth Vepakomma and Gupta, Otkrist and Raskar, Ramesh},
  title   = {Detailed comparison of communication efficiency of split learning and federated learning},
  journal = {preprint arXiv:1909.09145v1},
  year    = {2019},
}

@Article{FL-exp-reach2018,
  author  = {Caldas, Sebastian and Kone\v{c}n\'{y}, Jakub and McMahan, H. Brendan and Talwalkar, Ameet},
  title   = {EXPANDING THE REACH OF FEDERATED LEARNING BY REDUCING CLIENT RESOURCE REQUIREMENTS},
  journal = {preprint arXiv:1812.07210},
  year    = {2018},
}

@InProceedings{localSGD,
  author    = {Khaled, Ahmed and Mishchenko, Konstantin and Richt\'{a}rik, Peter},
  title     = {Better communication complexity for local {SGD}},
  booktitle = {NeurIPS Workshop on Federated Learning for Data Privacy and Confidentiality},
  year      = {2019},
}

@Article{FL-FedProx,
  author  = {Li, Tian Li and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  title   = {Federated Optimization for Heterogeneous Networks},
  journal = {preprint arXiv:1812.06127},
  year    = {2018},
}

@InCollection{NIPS2017_FL-multitask,
  author    = {Smith, Virginia and Chiang, Chao-Kai and Sanjabi, Maziar and Talwalkar, Ameet S},
  title     = {Federated Multi-Task Learning},
  booktitle = {Advances in Neural Information Processing Systems 30},
  publisher = {Curran Associates, Inc.},
  year      = {2017},
  editor    = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {4424--4434},
  url       = {http://papers.nips.cc/paper/7029-federated-multi-task-learning.pdf},
}

@Misc{FLblog2017,
  author       = {McMahan, Brendan and Ramage, Daniel},
  title        = {Federated Learning: Collaborative Machine Learning without Centralized Training Data},
  howpublished = {GoogleAIBlog},
  month        = apr,
  year         = {2017},
}

@Unknown{SCAFFOLD,
  author = {Karimireddy, Sai and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda},
  title  = {{SCAFFOLD}: Stochastic Controlled Averaging for On-Device Federated Learning},
  month  = {10},
  year   = {2019},
}

@Article{FL2019variational,
  author  = {L. Corinzia and J. M. Buhmann},
  title   = {Variational federated multi-task learning},
  journal = {preprint arXiv:1906.06268},
  year    = {2019},
}
@unpublished{Chraibi2019DistributedFP,
  title={Distributed Fixed Point Methods with Compressed Iterates},
  author={S. Chraibi and Ahmed Khaled and Dmitry Kovalev and Peter Richt{\'a}rik and Adil Salim and Martin Tak\'{a}\v{c}},
  journal={preprint ArXiv:1912.09925},
  year={2019},
}

@Article{Zhao2018FL-transfer-learn,
  author  = {Y. Zhao and M. Li and L. Lai and N. Suda and D. Civin and V. Chandra},
  title   = {Federated learning with non-iid data},
  journal = {preprint arXiv:1806.00582},
  year    = {2018},
}

@article{khaled2019better,
    title={{Better Communication Complexity for Local SGD}},
    author={Ahmed Khaled and Konstantin Mishchenko and Peter Richt{\'a}rik},
    year={2019},
    journal={preprint arXiv:1909.04746}
}

@article{con19,
	author = "L. Condat and D. Kitahara and A. Contreras and A. Hirabayashi",
	title = "Proximal Splitting Algorithms for Convex Optimization: {A} Tour of Recent Advances, with New Twists", 
	year = "2022",
	journal = "SIAM Review",
	note = "to appear"
}

@book{bau11a,
	editor = "H. H. Bauschke and R. S. Burachik and P. L. Combettes and V. Elser and D. R. Luke and H. Wolkowicz",
	title = "Fixed-Point Algorithms for Inverse Problems in Science and Engineering",
	publisher = "Springer",
	year = "2011",
}

@book{bau17,
	author = "H. H. Bauschke and P. L. Combettes",
	title = "Convex Analysis and Monotone Operator Theory in Hilbert Spaces",
	publisher = "Springer",
	address = "New York",
	year = "2017",
	edition = "2nd"
}

@article{khaled2019analysis,
    title={{First Analysis of Local GD on Heterogeneous Data}},
    author={Ahmed Khaled and Konstantin Mishchenko and Peter Richt{\'a}rik},
    year={2019},
    journal={preprint arXiv:1909.04715}
}

@article{Bottou18,
author = {Bottou, L{\'e}on. and Curtis, Frank E. and Nocedal, Jorge.},
title = {{Optimization Methods for Large-Scale Machine Learning}},
journal = {SIAM Review},
volume = {60},
number = {2},
pages = {223-311},
year = {2018},
doi = {10.1137/16M1080173}
}

@incollection{Zhang13,
title = {{Information-theoretic lower bounds for distributed statistical estimation with communication constraints}},
author = {Zhang, Yuchen and Duchi, John and Jordan, Michael I. and Wainwright, Martin J.},
booktitle = {Advances in Neural Information Processing Systems 26},
pages = {2328--2336},
year = {2013}
}


@article{Mangasarian95,
author = {Olvi L. Mangasarian},
title = {{Parallel Gradient Distribution in Unconstrained Optimization}},
journal = {SIAM Journal on Control and Optimization},
volume = {33},
number = {6},
pages = {1916-1925},
year = {1995}
}

@phdthesis{Coppola15,
  author    = {Gregory F. Coppola},
  title     = {{Iterative parameter mixing for distributed large-margin training of
               structured predictors for natural language processing}},
  school    = {University of Edinburgh, {UK}},
  year      = {2015},
  timestamp = {Mon, 08 May 2017 17:34:36 +0200},
  biburl    = {https://dblp.org/rec/bib/phd/ethos/Coppola15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{McDonald10,
 author = {McDonald, Ryan and Hall, Keith and Mann, Gideon},
 title = {{Distributed Training Strategies for the Structured Perceptron}},
 booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
 series = {HLT '10},
 year = {2010},
 isbn = {1-932432-65-5},
 pages = {456--464},
 numpages = {9}
}





@unpublished{kon16,
  author        = {J. Kone\v{c}n\'{y} and H. B. McMahan and F. X. Yu and P. Richt\'{a}rik and A. T. Suresh and D. Bacon},
  title         = "Federated learning: {S}trategies for improving communication efficiency",
  note     = "Paper arXiv:1610.05492, presented at the NIPS Workshop on Private Multi-Party Machine Learning",
  year          = {2016},
}


@article{Caldas18,
  author    = {Sebastian Caldas and
               Jakub Kone\v{c}n{\'{y}} and
               H. Brendan McMahan and
               Ameet Talwalkar},
  title     = {{Expanding the Reach of Federated Learning by Reducing Client Resource
               Requirements}},
  journal   = {preprint arXiv:1812.07210},
  year      = {2018}
}

@article{Dekel10,
  author    = {Ofer Dekel and
               Ran Gilad{-}Bachrach and
               Ohad Shamir and
               Lin Xiao},
  title     = {{Optimal Distributed Online Prediction using Mini-Batches}},
  journal   = {preprint arXiv:1012.1367},
  year      = {2010}
}



@incollection{Basu2019,
title = {{Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification and Local Computations}},
author = {Basu, Debraj and Data, Deepesh and Karakus, Can and Diggavi, Suhas},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {14668--14679},
year = {2019}
}

@incollection{Jiang18,
title = {{A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication}},
author = {Jiang, Peng and Agrawal, Gagan},
booktitle = {Advances in Neural Information Processing Systems 31},
pages = {2525--2536},
year = {2018}
}

@techreport{Wang2019,
archivePrefix = {arXiv},
arxivId = {1810.08313},
author = {Wang, Jianyu and Joshi, Gauri},
eprint = {1810.08313},
file = {:mnt/01D35D8F61D34AA0/Learning/Research/Wang, Joshi - 2019 - Adaptive Communication Strategies to achieve the best error-runtime trade-off in Local-Update SGD.pdf:pdf},
title = {{Adaptive Communication Strategies to Achieve the Best Error-Runtime Trade-off in Local-Update SGD}},
year = {2018}
}

@incollection{Patel19,
title = {{Communication trade-offs for Local-SGD with large step size}},
author = {Dieuleveut, Aymeric and Patel, Kumar Kshitij},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {13579--13590},
year = {2019}
}

@article{Wang18,
Author = {Jianyu Wang and Gauri Joshi},
Title = {{Cooperative SGD: A Unified Framework for the Design and Analysis of Communication-Efficient SGD Algorithms}},
Year = {2018},
journal = {preprint arXiv:1808.07576}
}
@inproceedings{TaoLin19,
title={{Don't Use Large Mini-batches, Use Local SGD}},
author={Tao Lin and Sebastian U. Stich and Kumar Kshitij Patel and Martin Jaggi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=B1eyO1BFPr}
}

@inproceedings{Li2019,
title={{On the Convergence of FedAvg on Non-IID Data}},
author={Xiang Li and Kaixuan Huang and Wenhao Yang and Shusen Wang and Zhihua Zhang},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HJxNAnVtDS}
}

@inproceedings{Stich2018,
	title={{Local SGD Converges Fast and Communicates Little}},
	author={Sebastian U. Stich},
	booktitle={International Conference on Learning Representations},
	year={2019}
}
@article{Zhou18,
   title={{On the Convergence Properties of a K-step Averaging Stochastic Gradient Descent Algorithm for Nonconvex Optimization}},
   journal={Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence},
   author={Zhou, Fan and Cong, Guojing},
   year={2018}
}

@article{Yu18,
   title={{Parallel Restarted SGD with Faster Convergence and Less Communication: Demystifying Why Model Averaging Works for Deep Learning}},
   volume={33},
   journal={Proceedings of the AAAI Conference on Artificial Intelligence},
   author={Yu, Hao and Yang, Sen and Zhu, Shenghuo},
   pages={5693–5700},
   year={2019}
}

@book{Beck2017,
author = {Beck, A.},
title = {First-Order Methods in Optimization},
publisher = {Society for Industrial and Applied Mathematics},
year = {2017},
doi = {10.1137/1.9781611974997},
address = {Philadelphia, PA},
edition   = {},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611974997}
}


@Article{dalcin2011parallel,
  author    = {Dalcin, Lisandro D. and Paz, Rodrigo R. and Kler, Pablo A. and Cosimo, Alejandro},
  title     = {{Parallel distributed computing using Python}},
  journal   = {Advances in Water Resources},
  year      = {2011},
  volume    = {34},
  number    = {9},
  pages     = {1124--1139}
}


@article{stich19errorfeedback,
    title={{The Error-Feedback Framework: Better Rates for SGD with Delayed Gradients and Compressed Communication}},
    author={Sebastian U. Stich and Sai Praneeth Karimireddy},
    year={2019},
    journal={preprint arXiv:1909.05350}
}

@article{Reisizadeh19,
    title={{FedPAQ: A Communication-Efficient Federated Learning Method with Periodic Averaging and Quantization}},
    author={Amirhossein Reisizadeh and Aryan Mokhtari and Hamed Hassani and Ali Jadbabaie and Ramtin Pedarsani},
    year={2019},
    journal={preprint arXiv:1909.13014}
}

@article{WangTuor18,
  author    = {Shiqiang Wang and
               Tiffany Tuor and
               Theodoros Salonidis and
               Kin K. Leung and
               Christian Makaya and
               Ting He and
               Kevin Chan},
  title     = {{When Edge Meets Learning: Adaptive Control for Resource-Constrained
               Distributed Machine Learning}},
  journal   = {arXiv:1804.05271},
  year      = {2018},
  timestamp = {Mon, 10 Dec 2018 13:28:19 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1804-05271},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}







@InProceedings{yu2019linear,
  title = 	 {{On the Linear Speedup Analysis of Communication Efficient Momentum SGD for Distributed Non-Convex Optimization}},
  author = 	 {Yu, Hao and Jin, Rong and Yang, Sen},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  year = 	 {2019}
}

@article{wang2019slowmo,
    title={{SlowMo: Improving Communication-Efficient Distributed SGD with Slow Momentum}},
    author={Jianyu Wang and Vinayak Tantia and Nicolas Ballas and Michael Rabbat},
    year={2019},
    journal={preprint arXiv:1910.00643}
}

@article{liang2019variance,
    title={{Variance Reduced Local SGD with Lower Communication Complexity}},
    author={Xianfeng Liang and Shuheng Shen and Jingchang Liu and Zhen Pan and Enhong Chen and Yifei Cheng},
    year={2019},
    journal={preprint arXiv:1912.12844}
}

@article{karimireddy2019scaffold,
    title={{SCAFFOLD: Stochastic Controlled Averaging for On-Device Federated Learning}},
    author={Sai Praneeth Karimireddy and Satyen Kale and Mehryar Mohri and Sashank J. Reddi and Sebastian U. Stich and Ananda Theertha Suresh},
    year={2019},
    journal={preprint arXiv:1910.06378}
}

@article{xie2019local,
    title={{Local AdaAlter: Communication-Efficient Stochastic Gradient Descent with Adaptive Learning Rates}},
    author={Cong Xie and Oluwasanmi Koyejo and Indranil Gupta and Haibin Lin},
    year={2019},
    journal={preprint arXiv:1911.09030}
}

@article{sharma2019parallel,
    title={{Parallel Restarted SPIDER -- Communication Efficient Distributed Nonconvex Optimization with Optimal Computation Complexity}},
    author={Pranay Sharma and Prashant Khanduri and Saikiran Bulusu and Ketan Rajawat and Pramod K. Varshney},
    year={2019},
    journal={preprint arXiv:1912.06036}
}

@inproceedings{mishchenko2018delay,
  title={{A Delay-tolerant Proximal-Gradient Algorithm for Distributed Learning}},
  author={Mishchenko, Konstantin and Iutzeler, Franck and Malick, J{\'e}r{\^o}me and Amini, Massih-Reza},
  booktitle={International Conference on Machine Learning},
  pages={3584--3592},
  year={2018}
}

@article{mishchenko2019revisiting,
  title={{Revisiting Stochastic Extragradient}},
  author={Mishchenko, Konstantin and Kovalev, Dmitry and Shulgin, Egor and Richt{\'a}rik, Peter and Malitsky, Yura},
  journal = {to appear in the 23rd International Conference on Artificial
  Intelligence and Statistics (AISTATS)},
  year={2019}
}

@article{juditsky2011solving,
  title={{Solving variational Inequalities with Stochastic Mirror-Prox algorithm}},
  author={Juditsky, Anatoli and Nemirovski, Arkadi and Tauvel, Claire},
  journal={Stochastic Systems},
  volume={1},
  number={1},
  pages={17--58},
  year={2011}
}

@incollection{chavdarova2019reducing,
	title = {{Reducing Noise in GAN Training with Variance Reduced Extragradient}},
	author = {Chavdarova, Tatjana and Gidel, Gauthier and Fleuret, Fran\c{c}ois and Lacoste-Julien, Simon},
	booktitle = {Advances in Neural Information Processing Systems 32},
	pages = {391--401},
	year = {2019},
	publisher = {Curran Associates, Inc.}
}


@article{Sahu18,
    title={{Federated Optimization in Heterogeneous Networks}},
    author={Tian Li and Anit Kumar Sahu and Manzil Zaheer and Maziar Sanjabi and Ameet Talwalkar and Virginia Smith},
    year={2018},
    journal={preprint arXiv:1812.06127}
}


@article{HardRao18,
  author    = {Andrew Hard and
               Kanishka Rao and
               Rajiv Mathews and
               Fran{\c{c}}oise Beaufays and
               Sean Augenstein and
               Hubert Eichner and
               Chlo{\'{e}} Kiddon and
               Daniel Ramage},
  title     = {{Federated Learning for Mobile Keyboard Prediction}},
  journal   = {preprint arXiv:1811.03604},
  year      = {2018},
  timestamp = {Fri, 23 Nov 2018 12:43:51 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1811-03604},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{zhang2019distributed,
    title={{Distributed Optimization for Over-Parameterized Learning}},
    author={Chi Zhang and Qianxiao Li},
    year={2019},
    journal={preprint arXiv:1906.06205}
}

@inproceedings{Haddadpour2019local,
title = {{Local SGD with  Periodic Averaging: Tighter Analysis  and Adaptive Synchronization}},
author = {Haddadpour, Farzin and Kamani, Mohammad Mahdi and Mahdavi, Mehrdad and Cadambe, Viveck},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {11080--11092},
year = {2019}
}


@article{Haddadpour19FL,
    title={{On the Convergence of Local Descent Methods in Federated Learning}},
    author={Farzin Haddadpour and Mehrdad Mahdavi},
    year={2019},
    journal={preprint arXiv:1910.14425}
}

@InProceedings{MAML2017,
  author    = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  title     = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  booktitle = {International Conference on Machine Learning},
  year      = {2017},
}

@Article{FL2019variational,
  author  = {L. Corinzia and J. M. Buhmann},
  title   = {Variational federated multi-task learning},
  journal = {preprint arXiv:1906.06268},
  year    = {2019},
}

@InProceedings{Eichner2019semi-cyclicSGD_for_FL,
  author    = {H. Eichner and T. Koren and H. B. McMahan and N. Srebro and K. Talwar},
  title     = {Semi-cyclic stochastic gradient descent},
  booktitle = {International Conference on Machine Learning},
  year      = {2019},
}

@Article{Zhao2018FL-transfer-learn,
  author  = {Y. Zhao and M. Li and L. Lai and N. Suda and D. Civin and V. Chandra},
  title   = {Federated learning with non-iid data},
  journal = {preprint arXiv:1806.00582},
  year    = {2018},
}

@InProceedings{Khodak2019FLmeta,
  author  = {M. Khodak and M.-F. Balcan and A. Talwalkar},
  title   = {Adaptive gradient-based meta-learning methods},
  year    = {2019},
  booktitle = {Advances in Neural Information Processing Systems 32},
}

@unpublished{kha20,
  author  = "A. Khaled and O. Sebbouh and N. Loizou and R. M. Gower M. and P. {Richt\'{a}rik}",
  note = {arXiv:2006.11573},
  title   = {Unified Analysis of Stochastic Gradient Methods for Composite Convex and Smooth Optimization},
  year    = {2020},
}

@conference{gor202,
  author    = "E. Gorbunov  and F. Hanzely and P. {Richt\'{a}rik}",
  title     = "A Unified Theory of {SGD: Variance} Reduction, Sampling, Quantization and Coordinate Descent",
  booktitle ="Proc. of 23rd Int. Conf. Artificial Intelligence and Statistics (AISTATS)",
  year      = {2020},
}



@inproceedings{allen2017katyusha,
  title={Katyusha: The first direct acceleration of stochastic gradient methods},
  author={Allen-Zhu, Zeyuan},
  booktitle={Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing},
  pages={1200--1205},
  year={2017},
  organization={ACM}
}

@book{NesterovBook,
  title     = {Introductory lectures on convex optimization: a basic course},
  publisher = {Kluwer Academic Publishers},
  year      = {2004},
  author    = {Yurii Nesterov},
}

@InProceedings{IProx-SDCA,
  author    = {Zhao, Peilin and Zhang, Tong},
  title     = {Stochastic optimization with importance sampling for regularized loss minimization},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning, PMLR},
  year      = {2015},
  volume    = {37},
  pages     = {1--9},
  journal   = {The 32nd International Conference on Machine Learning},
}

@InProceedings{pmlr-v97-qian19b,
  title = 	 {{SGD}: General Analysis and Improved Rates},
  author = 	 {Gower, Robert Mansel and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richt{\'a}rik, Peter},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5200--5209},
  year = 	 {2019},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  url = 	 {http://proceedings.mlr.press/v97/qian19b.html},
}

@Article{AIDE,
  author        = {Reddi, Sashank J. and Kone\v{c}n\'{y}, Jakub and Richt\'{a}rik, Peter and P\'{o}czos, Barnab\'{a}s and Smola, Alex},
  title         = {{AIDE}: fast and communication efficient distributed optimization},
  journal       = {arXiv:1608.06879},
  year          = {2016},
}

@InProceedings{Dean2012,
  author    = {Jeffrey Dean and Greg Corrado and Rajat Monga and Kai Chen and Matthieu Devin and Mark Mao and Andrew Senior and Paul Tucker and Ke Yang and Quoc V Le and et al},
  title     = {Large scale distributed deep networks},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2012},
  pages     = {1223--1231},
}


@article{mishchenko2019stochastic,
  title={A stochastic decoupling method for minimizing the sum of smooth and non-smooth functions},
  author={Mishchenko, Konstantin and Richt{\'a}rik, Peter},
  journal={preprint arXiv:1905.11535},
  year={2019}
}

@InProceedings{gazagnadou2019optimal,
  title={Optimal mini-batch and step sizes for {SAGA}},
  author={Gazagnadou, Nidham and Gower, Robert M and Salmon, Joseph},
  booktitle={International Conference on Machine Learning},
  year={2019}
}

@Article{FL-big,
  author  = {Kairouz, Peter and McMahan, H. Brendan and et al},
  title   = {Advances and Open Problems in Federated Learning},
  journal = {preprint arXiv:1912.04977v1},
  year    = {2019},
}

@Article{FL-FedProx,
  author  = {Li, Tian Li and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  title   = {Federated Optimization for Heterogeneous Networks},
  journal = {preprint arXiv:1812.06127},
  year    = {2018},
}

% and Avent, Brendan and Bellet, Aur\'{e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Keith and Charles, Zachary and Cormode, Graham and Cummings, Rachel and D’Oliveira, Rafael G.L. and El Rouayheb, Salim and Evans, David and Gardner, Josh and Gasc\'{o}n, Adri\`{a} and Ghazi, Badih and Gibbons, Phillip B. and Garrett, Zachary et al

@InCollection{NIPS2017_FL-multitask,
  author    = {Smith, Virginia and Chiang, Chao-Kai and Sanjabi, Maziar and Talwalkar, Ameet S},
  title     = {Federated Multi-Task Learning},
  booktitle = {Advances in Neural Information Processing Systems 30},
  publisher = {Curran Associates, Inc.},
  year      = {2017},
  editor    = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {4424--4434},
  url       = {http://papers.nips.cc/paper/7029-federated-multi-task-learning.pdf},
}

@Article{FL_survey_2019,
  author  = {Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia},
  title   = {Federated learning: challenges, methods, and future directions},
  journal = {preprint arXiv:1908.07873},
  year    = {2019},
}



@InProceedings{localSGD-AISTATS2020,
  author    = {Khaled, Ahmed and Mishchenko, Konstantin and Richt\'{a}rik, Peter},
  title     = {Tighter theory for local {SGD} on identical and heterogeneous data},
  booktitle = {The 23rd International Conference on Artificial Intelligence and Statistics (AISTATS 2020)},
  year      = {2020},
}

@InProceedings{localSGD,
  author    = {Khaled, Ahmed and Mishchenko, Konstantin and Richt\'{a}rik, Peter},
  title     = {Better communication complexity for local {SGD}},
  booktitle = {NeurIPS Workshop on Federated Learning for Data Privacy and Confidentiality},
  year      = {2019},
}



@InProceedings{localSGD-Stich,
  author    = {Stich, Sebastian U.},
  title     = {Local {SGD} Converges Fast and Communicates Little},
  booktitle = {International Conference on Learning Representations},
  year      = {2020},
}



@Article{FedAvg2016,
  author  = {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Ag\"{u}era y Arcas, Blaise},
  title   = {Federated Learning of Deep Networks using Model Averaging},
  journal = {preprint arXiv:1602.05629},
  year    = {2016},
}

@Article{FEDOPT,
  author        = {Kone\v{c}n\'{y}, Jakub and McMahan, H. Brendan and Ramage, Daniel and Richt\'{a}rik, Peter},
  title         = {Federated optimization: distributed machine learning for on-device intelligence},
  journal       = {preprint arXiv:1610.02527},
  year          = {2016},
}





@inproceedings{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={315--323},
  year={2013}
}

@article{wu2019federated,
  title={Federated Variance-Reduced Stochastic Gradient Descent with Robustness to Byzantine Attacks},
  author={Wu, Zhaoxian and Ling, Qing and Chen, Tianyi and Giannakis, Georgios B},
  journal={preprint arXiv:1912.12716},
  year={2019}
}

@article{liang2019variance,
  title={Variance Reduced Local {SGD} with Lower Communication Complexity},
  author={Liang, Xianfeng and Shen, Shuheng and Liu, Jingchang and Pan, Zhen and Chen, Enhong and Cheng, Yifei},
  journal={preprint arXiv:1912.12844},
  year={2019}
}

@article{karimireddy2019scaffold,
  title={{SCAFFOLD:} Stochastic controlled averaging for on-device federated learning},
  author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank J and Stich, Sebastian U and Suresh, Ananda Theertha},
  journal={preprint arXiv:1910.06378},
  year={2019}
}

@inproceedings{hofmann2015variance,
  title={Variance reduced stochastic gradient descent with neighbors},
  author={Hofmann, Thomas and Lucchi, Aurelien and Lacoste-Julien, Simon and McWilliams, Brian},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2305--2313},
  year={2015}
}



@Article{JacSketch,
  author        = {Gower, Robert Mansel and Richt\'{a}rik, Peter and Bach, Francis},
  title         = {Stochastic quasi-gradient methods: variance reduction via {J}acobian sketching},
  journal       = {preprint arXiv:1805.02632},
  year          = {2018},
}

@InProceedings{qian2019saga,
  author    = {Qian, Xun and Qu, Zheng and Richt\'{a}rik, Peter},
  title     = {{SAGA} with arbitrary sampling},
  booktitle = {The 36th International Conference on Machine Learning},
  year      = {2019},
}



@InProceedings{kovalev2019don,
  author    = {Kovalev, Dmitry and Horv\'{a}th, Samuel and Richt\'{a}rik, Peter},
  title     = {Don’t jump through hoops and remove those loops: {SVRG} and {K}atyusha are better without the outer loop},
  booktitle = {Proceedings of the 31st International Conference on Algorithmic Learning Theory},
  year      = {2020},
}



@article{ma2017distributed,
  title={Distributed optimization with arbitrary local solvers},
  author={Ma, Chenxin and Kone{\v{c}}n{\'y}, Jakub and Jaggi, Martin and Smith, Virginia and Jordan, Michael I and Richt{\'a}rik, Peter and Tak{\'a}{\v{c}}, Martin},
  journal={Optimization Methods and Software},
  volume={32},
  number={4},
  pages={813--848},
  year={2017},
  publisher={Taylor \& Francis}
}
@article{haddadpour2019convergence,
  title={On the Convergence of Local Descent Methods in Federated Learning},
  author={Haddadpour, Farzin and Mahdavi, Mehrdad},
  journal={preprint arXiv:1910.14425},
  year={2019}
}



