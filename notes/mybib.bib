@article{mishchenko2022proxskip,
  title={ProxSkip: Yes! Local Gradient Steps Provably Lead to Communication Acceleration! Finally!},
  author={Mishchenko, Konstantin and Malinovsky, Grigory and Stich, Sebastian and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2202.09357},
  year={2022}
}

@article{fatkhullin2021ef21,
  title={EF21 with bells \& whistles: Practical algorithmic extensions of modern error feedback},
  author={Fatkhullin, Ilyas and Sokolov, Igor and Gorbunov, Eduard and Li, Zhize and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2110.03294},
  year={2021}
}

@inproceedings{kovalev2020don,
  title={Donâ€™t jump through hoops and remove those loops: SVRG and Katyusha are better without the outer loop},
  author={Kovalev, Dmitry and Horv{\'a}th, Samuel and Richt{\'a}rik, Peter},
  booktitle={Algorithmic Learning Theory},
  pages={451--467},
  year={2020},
  organization={PMLR}
}

@article{qian2021svrg,
  title={L-SVRG and L-Katyusha with Arbitrary Sampling},
  author={Qian, Xun and Qu, Zheng and Richt{\'a}rik, Peter},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={112},
  pages={1--47},
  year={2021}
}

@article{chang2011libsvm,
  title={LIBSVM: a library for support vector machines},
  author={Chang, Chih-Chung and Lin, Chih-Jen},
  journal={ACM transactions on intelligent systems and technology (TIST)},
  volume={2},
  number={3},
  pages={1--27},
  year={2011},
  publisher={Acm New York, NY, USA}
}

@article{condat2022ef,
  title={EF-BV: A Unified Theory of Error Feedback and Variance Reduction Mechanisms for Biased and Unbiased Compression in Distributed Optimization},
  author={Condat, Laurent and Yi, Kai and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2205.04180},
  year={2022}
}


@unpublished{Cnat,
  author        = {Horv\'{a}th, Samuel and Ho, Chen-Yu and Horv\'{a}th, Ludov\'{i}t and Sahu, Atal Narayan and Canini, Marco and Richt\'{a}rik, Peter},
  title         = {Natural Compression for Distributed Deep Learning},
  note       = {preprint arXiv:1905.10988},
  year          = {2019},
}

@InProceedings{kar16,
author="Karimi, Hamed
and Nutini, Julie
and Schmidt, Mark",
editor="Frasconi, Paolo
and Landwehr, Niels
and Manco, Giuseppe
and Vreeken, Jilles",
title="Linear Convergence of Gradient and Proximal-Gradient Methods Under the {P}olyak-{\L}ojasiewicz Condition",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="795--811",
}

@unpublished{sze22,
author = "R. Szlendak and A. Tyurin and P. Richt{\'a}rik",
title = "Permutation Compressors for Provably Faster Distributed Nonconvex Optimization", 
year = "2021",
note = "preprint arXiv:2110.03300, accepted at ICLR 2022"
}



@article{att09,
author = "H. Attouch and J. Bolte",
title = "On the convergence of the proximal algorithm for nonsmooth functions involving analytic features", 
journal = "Math. Program.",
year = "2009",
volume = "116",
pages = "5--116"
}

@article{con22,
author = "L. Condat and G. Malinovsky and P. Richt{\'a}rik",
title = "Distributed Proximal Splitting Algorithms with Rates and Acceleration", 
journal = "Frontiers in Signal Processing",
year = "2022",
volume = "1",
month = jan
}


@conference{mis20,
	author = "Konstantin Mishchenko and Filip Hanzely and Peter Richt{\'a}rik",
	title = "99\% of worker-master communication in distributed optimization is not needed",
	booktitle = "Proc. of 36th Conf. on Uncertainty in Artificial Intelligence (UAI)",
	volume ="124", 
	pages ="979--988",
	year = "2020",
}


@article{saf21,
author = "Mher Safaryan and Egor Shulgin and Peter Richt{\'a}rik",
title = "Uncertainty principle for communication compression in distributed and federated learning and the search for an optimal compressor", 
journal = "Information and Inference: A Journal of the IMA",
year = "2021",
}

@unpublished{con21,
  author  = "L. Condat and P. {Richt\'{a}rik}",
  title   = "{MURANA: A} Generic Framework for Stochastic Variance-Reduced Optimization",
 note = {preprint arXiv:2106.03056v1},
  year    = {2021},
}


@article{ver21,
author = "J. Verbraeken and M. Wolting and J. Katzy and J. Kloppenburg and T. Verbelen and J. S. Rellermeyer",
title = "A survey on distributed machine learning", 
journal = "ACM Computing Surveys",
year = "2021",
month = mar,
number = "2",
volume = "53",
pages = "1--33"
}

@article{kai19,
author = "P. Kairouz and others",
title = "Advances and Open Problems in Federated Learning", 
journal = "Foundations and Trends in Machine Learning",
year = "2021",
number = "1--2",
volume = "14"
}

@article{han19,
  title={One Method to Rule Them All: Variance Reduction for Data, Parameters and Many New Methods},
  author={Hanzely, Filip and Richt{\'a}rik, Peter},
  journal={preprint arXiv:1905.11266},
  year={2019}
}


@unpublished{xu20,
	author = "H. Xu and C.-Y. Ho and A. M. Abdelmoniem and A. Dutta and E. H. Bergou and K. Karatsenidis and M. Canini and P. Kalnis",
	title = "Compressed communication for distributed deep learning: {S}urvey and quantitative evaluation",
	year = "2020",
	note = "Technical report, KAUST"
}

@conference{xu21,
	author = "H. Xu and C.-Y. Ho and A. M. Abdelmoniem and A. Dutta and E. H. Bergou and K. Karatsenidis and M. Canini and P. Kalnis",
	title = "{GRACE: A compressed communication framework for distributed machine learning}",
	booktitle = "Proc. of 41st IEEE Int. Conf. Distributed Computing Systems (ICDCS)",
	year = "2021",
}


@conference{mcm17,
  author    = {McMahan, H Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and Ag\"{u}era y Arcas, Blaise},
  title     = {Communication-efficient learning of deep networks from decentralized data},
  booktitle = {Proc. of Int. Conf. Artificial Intelligence and Statistics (AISTATS)},
  year      = {2017},
  volume = "PMLR 54",
}


@conference{def14,
  title="{SAGA: A} fast incremental gradient method with support for non-strongly convex composite objectives",
  author="A. Defazio and F. Bach and S. {Lacoste-Julien}",
  booktitle="Proc. of 28th Conf. Neural Information Processing Systems (NIPS)",
  pages={1646--1654},
  year={2014}
}

@conference{sei14,
title = "1-Bit Stochastic Gradient Descent and Application to Data-Parallel Distributed Training of Speech {DNNs}",
author = {Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
booktitle = {Proc. of Annual Conf. of  Int. Speech Communication Association (Interspeech)},
year = {2014},
}

@conference{ric21,
title = "{EF21}: {A} New, Simpler, Theoretically Better, and Practically Faster Error Feedback",
author = "Peter Richt{\'a}rik and Igor Sokolov and Ilyas Fatkhullin",
booktitle = "Proc. of 35th Conf. Neural Information Processing Systems (NeurIPS)",
year = {2021},
}

@unpublished{fat21,
  title="{EF21} with Bells {\&} Whistles: Practical Algorithmic Extensions of Modern Error Feedback",
  author="I. Fatkhullin and I. Sokolov and E. Gorbunov and Z. Li and P. Richt{\'a}rik",
  note = "preprint arXiv:2110.03294",
  year={2021}
}

@conference{ja2016,
title	= {Federated Learning: Strategies for Improving Communication Efficiency},
author	= {Jakub Kone\v{c}n\'{y} and H. Brendan McMahan and Felix X. Yu and Peter Richt\'{a}rik and Ananda Theertha Suresh and Dave Bacon},
year	= {2016},
booktitle	= {NIPS Workshop on Private Multi-Party Machine Learning}
}


@unpublished{qia19,
  title="{MISO} is Making a Comeback With Better Proofs and Rates",
  author="X. Qian and A. Sailanbayev and K. Mishchenko and P. Richt{\'a}rik",
  year={2019},
  month = jun,
  note = "arXiv:1906.01474"
}

@conference{gaz19,
  title="Optimal Mini-Batch and Step Sizes for {SAGA}",
  author="N. Gazagnadou and R. Gower and J. Salmon",
  booktitle="Proc. of 36th Int. Conf. Machine Learning (ICML)",
  pages="2142--2150",
  volume = "PMLR 97",
  year="2019"
}

@conference{gow19,
  author    = "R. M. Gower and N. Loizou and X. Qian and A. Sailanbayev and E. Shulgin and P. Richt\'{a}rik",
  title     = "{SGD}: {G}eneral Analysis and Improved Rates",
  booktitle="Proc. of 36th Int. Conf. Machine Learning (ICML)",  year      = {2019},
  volume = "PMLR 97",
  pages     = {5200--5209}
}

@conference{seb19,
  author    = "O. Sebbouh and N. Gazagnadou and S. Jelassi and F. Bach and R. Gower",
  title     = "Towards closing the gap between the theory and practice of {SVRG}",
  year={2019},
   booktitle = "Proc. of 33rd Conf. Neural Information Processing Systems (NeurIPS)",
}
 

@unpublished{bac21,
  title="Learning Theory from First Principles",
  author="F. Bach",
  note = "Draft of a book, version of Sept. 6, 2021",
  year={2021}
}


@conference{liu20,
  title="A double residual compression algorithm for efficient distributed learning",
  author="X. Liu and Y. Li and J. Tang and M. Yan",
  booktitle="Proc. of 23rd Int. Conf. Artificial Intelligence and Statistics (AISTATS)",
  pages="133--143",
  volume = "PMLR 108",
  year={2020}
}

@unpublished{phi20,
  title="Bidirectional compression in heterogeneous settings for distributed or federated learning with partial participation: tight convergence guarantees",
  author="C. Philippenko and A. Dieuleveut",
    year={2020},
    note = "arXiv:2006.14591"
}

@conference{tan19,
  title="DoubleSqueeze: {P}arallel stochastic gradient descent with double-pass error-compensated compression",
  author="H. Tang and C. Yu and X. Lian and T. Zhang and J. Liu",
  booktitle="Proc. of Int. Conf. Machine Learning (ICML)",
  pages="6155--6165",
  year={2019}
}


@conference{kov20,
  title="Don't Jump Through Hoops and Remove Those Loops: {SVRG and Katyusha} are Better Without the Outer Loop",
  author="D. Kovalev and S. {Horv\'ath} and P. {Richt\'arik}",
  booktitle="Proc. of 31st Int. Conf. Algorithmic Learning Theory (ALT)",
  volume = "PMLR 117",
  pages="451--467",
  year={2020}
}

@article{qia21,
  title="{L-SVRG and L-Katyusha} with Arbitrary Sampling",
  author="X. Qian and Z. Qu and P. {Richt\'arik}",
   year={2021},
   pages = {1--47},
   volume = {22},
   number = {112},
   journal = {Journal of Machine Learning Research}
}

@conference{li21,
  title="{CANITA: Faster} Rates for Distributed Convex Optimization with Communication Compression",
  author="Z. Li and P. {Richt\'arik}",
   year={2021},
   booktitle = "Proc. of 35th Conf. Neural Information Processing Systems (NeurIPS)"
}

@conference{li2020,
  title="Acceleration for compressed gradient descent in distributed and federated optimization",
  author="Z. Li and D. Kovalev and X. Qian and P. {Richt\'arik}",
   year={2020},
  booktitle = "Proc. of 37th Int. Conf. Machine Learning (ICML)"
  }


@conference{gor20,
  title="Linearly Converging Error Compensated {SGD}",
  author="E. Gorbunov and D. Kovalev and D. Makarenko and P. {Richt\'arik}",
   year={2020},
   booktitle = "Proc. of 34th Conf. Neural Information Processing Systems (NeurIPS)",
}

@conference{zha13,
  author    = "L. Zhang and M. Mahdavi and R. Jin",
  title     = "Linear convergence with condition number independent access of full gradients",
    booktitle="Proc. of 27th Conf. Neural Information Processing Systems (NIPS)",
  year      = {2013},
}

@conference{joh13,
  author    = "R. Johnson and T. Zhang",
  title     = {Accelerating stochastic gradient descent using predictive variance reduction},
    booktitle="Proc. of 27th Conf. Neural Information Processing Systems (NIPS)",
  year      = {2013},
  pages     = {315--323},
}

@article{xia14,
  author  = {L. Xiao and T. Zhang},
  title   = {A proximal stochastic gradient method with progressive variance reduction},
  journal = "SIAM J.  Optim.",
  year    = {2014},
  volume  = {24},
  number  = {4},
  pages   = {2057--2075},
}


@Article{L-SVRG-AS,
  author  = {Qian, Xun and Qu, Zheng and Richt\'{a}rik, Peter},
  title   = {{L-SVRG} and {L-K}atyusha with arbitrary sampling},
  journal = {preprint arXiv:1906.01481},
  year    = {2019},
}


@conference{hof15,
	author = "T. Hofmann and
A. Lucchi and S. {Lacoste-Julien} and
B. {McWilliams}",
	title = "Variance Reduced Stochastic Gradient Descent with Neighbors",
	booktitle = "Proc. of 29th Conf. Neural Information Processing Systems (NIPS)",
	year = "2015",
	pages="1509--1519"
}

@conference{ali17,
	author = "Alistarh, D. and Grubic, D. and Li, J. and Tomioka, R. and Vojnovic, M.",
	title = "{QSGD: Communication-efficient SGD via gradient quantization and encoding}",
	booktitle = "Proc. of 31st Conf. Neural Information Processing Systems (NIPS)",
	year = "2017",
	pages="1709--1720"
}

@conference{ali18,
  author    = {Dan Alistarh and Torsten Hoefler and Mikael Johansson and Sarit Khirirat and Nikola Konstantinov and C. Renggli},
  title     = {The Convergence of Sparsified Gradient Methods},
  booktitle = "Proc. of Conf. Neural Information Processing Systems (NeurIPS)",
  year      = {2018},
location = "Montreal, Canada"
}

@unpublished{bez20,
  author    = "A. Beznosikov and S. Horv{\'a}th and P. Richt{\'a}rik and M. Safaryan",
  title     = "On biased compression for distributed learning",
note = "preprint arXiv:2002.12410",
  year      = {2020},
}


@article{sat20,
  author="F. Sattler and S. Wiedemann and {K.-R. M\"uller} and W. Samek",
  journal="IEEE Trans. Neural Networks and Learning Systems", 
  title="Robust and Communication-Efficient Federated Learning From Non-i.i.d. Data", 
  year={2020},
  volume = "31",
  number = "9",
  pages="3400--3413"}

@conference{wen17,
	author = "Wen, W. and Xu, C. and Yan, F. and Wu, C. and Wang, Y. and Chen, Y. and Li,
H.",
	title = "{TernGrad: Ternary} gradients to reduce communication in distributed deep learning",
	booktitle = "Proc. of 31st Conf. Neural Information Processing Systems (NIPS)",
	year = "2017",
	pages="1509--1519"
}

@unpublished{mis19,
title = "Distributed Learning with Compressed Gradient Differences",
author = "K. Mishchenko and E. Gorbunov and M. {Tak\'a\v{c}} and P. {Richt\'arik}",
note = "arXiv:1901.09269",
year = "2019"
}

@unpublished{hor19,
title = "Stochastic Distributed Learning with Gradient Quantization and Variance Reduction",
author = "S. {Horv\'ath} and D. Kovalev and K. Mishchenko and S. Stich and P. {Richt\'arik}",
note = "arXiv:1904.05115",
year = "2019"
}

@article{gow20a,
  author="R. M. Gower and M. Schmidt and F. Bach and P. {Richt\'arik}",
  journal="Proc. of the IEEE", 
  title="Variance-Reduced Methods for Machine Learning", 
  year={2020},
  month = nov,
  volume = "108",
  number = "11",
  pages="1968--1983"}
  

@article{ric16,
  author="P. {Richt\'arik} and M. {Tak\'a\v{c}}",
  journal="Math.  Program.", 
  title="Parallel coordinate descent methods for big data optimization", 
  year={2016},
  volume = "156",
  pages="433--484"}
  
  @article{gow20,
  author="R. M. Gower and P. {Richt\'arik} and F. Bach",
  journal="Math. Program.", 
  title="Stochastic quasi-gradient methods: {V}ariance reduction via {J}acobian sketching", 
  year={2021},
  month = jul,
  volume = "188",
  pages = "135--192"
 }
  
   
@conference{dut20,
	Author = "A. Dutta and E. H. Bergou and A. M. Abdelmoniem and C. Y. Ho and A. N. Sahu and M. Canini and P. Kalnis",
	title = "On the discrepancy between the theoretical analysis and practical implementations of compressed communication for distributed deep learning",
	booktitle = "Proc. of AAAI Conf. Artificial Intelligence",
	year = {2020},
	pages = "3817--3824"
}
%number = "4",
%volume = "34",

@unpublished{alb20,
author = "A. Albasyoni and M. Safaryan and L. Condat and P. {Richt\'arik}",
title = "Optimal Gradient Compression for Distributed and Federated Learning", 
note = "preprint arXiv:2010.03246",
year = "2020"
}

@conference{wan18,
	Author = "Wangni, J. and Wang, J. and Liu, J. and Zhang, T. ",
	title = "Gradient sparsification for communication-efficient distributed optimization",
	booktitle = "Proc. of 32nd Conf. Neural Information Processing Systems (NeurIPS)",
	year = {2018},
	pages = "1306--1316"
}







@article{bas20,
  author={Basu, D. and Data, D. and Karakus, C. and Diggavi, S. N.},
  journal={IEEE Journal on Selected Areas in Information Theory}, 
  title="{Qsparse-Local-SGD: Distributed SGD With Quantization, Sparsification, and Local Computations}", 
  year={2020},
  volume={1},
  number={1},
  pages={217--226}}
  

@article{dos15,
	author = "A. Chambolle and C. Dossal",
	title = "On the Convergence of the Iterates of the {"Fast Iterative Shrinkage/Thresholding Algorithm"}",
	journal = "J. Optim. Theory Appl.",
	year = "2015",
	volume="166",
	pages="968--982"
}


@ARTICLE{com21,
  author={P. L. Combettes and J. C. Pesquet},
  journal={IEEE Transactions on Signal Processing}, 
  title={Fixed Point Strategies in Data Science}, 
  year={2021},
  note = "To appear"
  }

@conference{mal20,
	author = "G. Malinovsky and D. Kovalev and E. Gasanov and L. Condat and P. Richt{\'a}rik",
	title = "From local {SGD} to local fixed point methods for federated learning",
	booktitle = "Proc. of 37th Int. Conf. Machine Learning (ICML)",
	year = "2020"
}

@article{li20,
	author = "T. Li and A. K. Sahu and A. Talwalkar and V. Smith",
	title = "Federated Learning: Challenges, Methods, and Future Directions",
	journal = "IEEE Signal Processing Magazine",
	year = "2020",
	number="37",
	volume="3",
	pages="50--60"
}

@article{par14,
	author = "N. Parikh and S. Boyd",
	title = "Proximal Algorithms",
	journal = "Foundations and Trends in Optimization",
	year = "2014",
	number="1",
	volume="3",
	pages="127--239"
}

@article{bub14,
	author = "S. Bubeck",
	title = "Convex optimization: {A}lgorithms and complexity",
	journal = "Foundations and Trends in Machine Learning",
	year = "2014",
	volume="8",
	pages="231--357"
}

@inproceedings{dav16,
	author="D. Davis and W. Yin",
	editor="R. Glowinski and S. J. Osher and W. Yin",
	title="Convergence rate analysis of several splitting schemes",
	booktitle="Splitting Methods in Communication, Imaging, Science, and Engineering",
	year="2016",
	publisher="Springer International Publishing",
	address="Cham",
	pages="115--163",
}

@conference{com21a,
	author="P. L. Combettes and Z. C. Woodstock",
	title="A Fixed Point Framework for Recovering Signals from Nonlinear Transformations",
	year="2021",
	booktitle = "Proc. of 28th European Signal Processing Conference (EUSIPCO)",
	pages = "2120--2124"
}


@article{ric14,
author="P. Richt{\'a}rik and M. Tak{\'a}{\v{c}}",
title="Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function",
journal="Math. Program.",
year="2014",
month=apr,
volume="144",
pages="1--38",
}
%

@article{les16,
	author="L. Lessard and B. Recht and A. Packards",
	title="Analysis and Design of Optimization Algorithms via Integral Quadratic Constraints",
	journal="SIAM J. Optim.",
	year="2016",
	pages="57--95",
	volume = "26",
	number = "1"
}

@article{pes15,
	author = "J.-C. Pesquet and A. Repetti",
	title = "A Class of Randomized Primal-Dual Algorithms for Distributed Optimization",
	journal = "J. Nonlinear Convex Anal.",
	year = "2015",
	number="16",
	volume="12",
	month=dec,
}

@article{comy15,
	author = "P. L. Combettes and I. Yamada",
	title = "Compositions and convex combinations of averaged nonexpansive operators",
	journal = "Journal of Mathematical Analysis and Applications",
	year = "2015",
	number="1",
	volume="425",
	pages = "55--70"
}

@conference{yu13,
	author = "Y.-L. Yu",
	title = "On decomposing the proximal map",
	booktitle = "Proc. of 26th Int. Conf. Neural Information Processing Systems (NIPS)",
	year = "2013",
	pages = "91--99"
}


@Article{FEDOPT,
  author        = {Kone\v{c}n\'{y}, Jakub and McMahan, H. Brendan and Ramage, Daniel and Richt\'{a}rik, Peter},
  title         = {Federated optimization: distributed machine learning for on-device intelligence},
  journal       = {arXiv:1610.02527},
  year          = {2016},
}


@InProceedings{localGD,
  author      = {Khaled, Ahmed and Mishchenko, Konstantin and Richt\'{a}rik, Peter},
  title       = {First Analysis of Local {GD} on Heterogeneous Data},
  booktitle   = {NeurIPS Workshop on Federated Learning for Data Privacy and Confidentiality},
  year        = {2019},
 }

@InProceedings{GDCI,
  author      = {Khaled, Ahmed and Richt\'{a}rik, Peter},
  title       = {Gradient Descent with Compressed Iterates},
  booktitle   = {NeurIPS Workshop on Federated Learning for Data Privacy and Confidentiality},
  year        = {2019},
}

@Article{FL_survey_2019,
  author  = {Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia},
  title   = {Federated learning: challenges, methods, and future directions},
  journal = {preprint arXiv:1908.07873},
  year    = {2019},
}


@Article{FedAvg2016,
  author  = {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Ag\"{u}era y Arcas, Blaise},
  title   = {Federated Learning of Deep Networks using Model Averaging},
  journal = {preprint arXiv:1602.05629},
  year    = {2016},
}

@Article{Leaf2018,
  author  = {Caldas, Sebastian and Wua, Peter and Lia, Tian and Kone\v{c}n\'{y} and McMahan, Brendan and Smith, Virginia and Talwalkar, Ameet},
  title   = {Leaf: a benchmark for federated settings},
  journal = {preprint arXiv:1812.01097},
  year    = {2018},
}

@Article{FedMetaRecom2018,
  author  = {Chen, Fei and Dong, Zhenhua and Li, Zhenguo and He, Xiuqiang},
  title   = {Federated meta-learning for recommendation},
  journal = {preprint arXiv:1802.07876},
  year    = {2018},
}


@InProceedings{FL-MAML-Jakub,
  author    = {Jiang, Yihan and Kone\v{c}n\'{y}, Jakub and Rush, Keith and Kannan, Sreeram},
  title     = {Improving Federated Learning Personalization via Model Agnostic Meta Learning},
  booktitle = {NeurIPS Workshop on Federated Learning for Data Privacy and Confidentiality},
  year      = {2019},
}

@Article{FLvsSL2019,
  author  = {Abhishek Singh, Praneeth Vepakomma and Gupta, Otkrist and Raskar, Ramesh},
  title   = {Detailed comparison of communication efficiency of split learning and federated learning},
  journal = {preprint arXiv:1909.09145v1},
  year    = {2019},
}

@Article{FL-exp-reach2018,
  author  = {Caldas, Sebastian and Kone\v{c}n\'{y}, Jakub and McMahan, H. Brendan and Talwalkar, Ameet},
  title   = {EXPANDING THE REACH OF FEDERATED LEARNING BY REDUCING CLIENT RESOURCE REQUIREMENTS},
  journal = {preprint arXiv:1812.07210},
  year    = {2018},
}

@InProceedings{localSGD,
  author    = {Khaled, Ahmed and Mishchenko, Konstantin and Richt\'{a}rik, Peter},
  title     = {Better communication complexity for local {SGD}},
  booktitle = {NeurIPS Workshop on Federated Learning for Data Privacy and Confidentiality},
  year      = {2019},
}

@Article{FL-FedProx,
  author  = {Li, Tian Li and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  title   = {Federated Optimization for Heterogeneous Networks},
  journal = {preprint arXiv:1812.06127},
  year    = {2018},
}

@InCollection{NIPS2017_FL-multitask,
  author    = {Smith, Virginia and Chiang, Chao-Kai and Sanjabi, Maziar and Talwalkar, Ameet S},
  title     = {Federated Multi-Task Learning},
  booktitle = {Advances in Neural Information Processing Systems 30},
  publisher = {Curran Associates, Inc.},
  year      = {2017},
  editor    = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {4424--4434},
  url       = {http://papers.nips.cc/paper/7029-federated-multi-task-learning.pdf},
}

@Misc{FLblog2017,
  author       = {McMahan, Brendan and Ramage, Daniel},
  title        = {Federated Learning: Collaborative Machine Learning without Centralized Training Data},
  howpublished = {GoogleAIBlog},
  month        = apr,
  year         = {2017},
}

@Unknown{SCAFFOLD,
  author = {Karimireddy, Sai and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda},
  title  = {{SCAFFOLD}: Stochastic Controlled Averaging for On-Device Federated Learning},
  month  = {10},
  year   = {2019},
}

@Article{FL2019variational,
  author  = {L. Corinzia and J. M. Buhmann},
  title   = {Variational federated multi-task learning},
  journal = {preprint arXiv:1906.06268},
  year    = {2019},
}
@unpublished{Chraibi2019DistributedFP,
  title={Distributed Fixed Point Methods with Compressed Iterates},
  author={S. Chraibi and Ahmed Khaled and Dmitry Kovalev and Peter Richt{\'a}rik and Adil Salim and Martin Tak\'{a}\v{c}},
  journal={preprint ArXiv:1912.09925},
  year={2019},
}

@Article{Zhao2018FL-transfer-learn,
  author  = {Y. Zhao and M. Li and L. Lai and N. Suda and D. Civin and V. Chandra},
  title   = {Federated learning with non-iid data},
  journal = {preprint arXiv:1806.00582},
  year    = {2018},
}

@article{khaled2019better,
    title={{Better Communication Complexity for Local SGD}},
    author={Ahmed Khaled and Konstantin Mishchenko and Peter Richt{\'a}rik},
    year={2019},
    journal={preprint arXiv:1909.04746}
}

@article{con19,
	author = "L. Condat and D. Kitahara and A. Contreras and A. Hirabayashi",
	title = "Proximal Splitting Algorithms for Convex Optimization: {A} Tour of Recent Advances, with New Twists", 
	year = "2022",
	journal = "SIAM Review",
	note = "to appear"
}

@book{bau11a,
	editor = "H. H. Bauschke and R. S. Burachik and P. L. Combettes and V. Elser and D. R. Luke and H. Wolkowicz",
	title = "Fixed-Point Algorithms for Inverse Problems in Science and Engineering",
	publisher = "Springer",
	year = "2011",
}

@book{bau17,
	author = "H. H. Bauschke and P. L. Combettes",
	title = "Convex Analysis and Monotone Operator Theory in Hilbert Spaces",
	publisher = "Springer",
	address = "New York",
	year = "2017",
	edition = "2nd"
}

@article{khaled2019analysis,
    title={{First Analysis of Local GD on Heterogeneous Data}},
    author={Ahmed Khaled and Konstantin Mishchenko and Peter Richt{\'a}rik},
    year={2019},
    journal={preprint arXiv:1909.04715}
}

@article{Bottou18,
author = {Bottou, L{\'e}on. and Curtis, Frank E. and Nocedal, Jorge.},
title = {{Optimization Methods for Large-Scale Machine Learning}},
journal = {SIAM Review},
volume = {60},
number = {2},
pages = {223-311},
year = {2018},
doi = {10.1137/16M1080173}
}

@incollection{Zhang13,
title = {{Information-theoretic lower bounds for distributed statistical estimation with communication constraints}},
author = {Zhang, Yuchen and Duchi, John and Jordan, Michael I. and Wainwright, Martin J.},
booktitle = {Advances in Neural Information Processing Systems 26},
pages = {2328--2336},
year = {2013}
}


@article{Mangasarian95,
author = {Olvi L. Mangasarian},
title = {{Parallel Gradient Distribution in Unconstrained Optimization}},
journal = {SIAM Journal on Control and Optimization},
volume = {33},
number = {6},
pages = {1916-1925},
year = {1995}
}

@phdthesis{Coppola15,
  author    = {Gregory F. Coppola},
  title     = {{Iterative parameter mixing for distributed large-margin training of
               structured predictors for natural language processing}},
  school    = {University of Edinburgh, {UK}},
  year      = {2015},
  timestamp = {Mon, 08 May 2017 17:34:36 +0200},
  biburl    = {https://dblp.org/rec/bib/phd/ethos/Coppola15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{McDonald10,
 author = {McDonald, Ryan and Hall, Keith and Mann, Gideon},
 title = {{Distributed Training Strategies for the Structured Perceptron}},
 booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
 series = {HLT '10},
 year = {2010},
 isbn = {1-932432-65-5},
 pages = {456--464},
 numpages = {9}
}





@unpublished{kon16,
  author        = {J. Kone\v{c}n\'{y} and H. B. McMahan and F. X. Yu and P. Richt\'{a}rik and A. T. Suresh and D. Bacon},
  title         = "Federated learning: {S}trategies for improving communication efficiency",
  note     = "Paper arXiv:1610.05492, presented at the NIPS Workshop on Private Multi-Party Machine Learning",
  year          = {2016},
}


@article{Caldas18,
  author    = {Sebastian Caldas and
               Jakub Kone\v{c}n{\'{y}} and
               H. Brendan McMahan and
               Ameet Talwalkar},
  title     = {{Expanding the Reach of Federated Learning by Reducing Client Resource
               Requirements}},
  journal   = {preprint arXiv:1812.07210},
  year      = {2018}
}

@article{Dekel10,
  author    = {Ofer Dekel and
               Ran Gilad{-}Bachrach and
               Ohad Shamir and
               Lin Xiao},
  title     = {{Optimal Distributed Online Prediction using Mini-Batches}},
  journal   = {preprint arXiv:1012.1367},
  year      = {2010}
}



@incollection{Basu2019,
title = {{Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification and Local Computations}},
author = {Basu, Debraj and Data, Deepesh and Karakus, Can and Diggavi, Suhas},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {14668--14679},
year = {2019}
}

@incollection{Jiang18,
title = {{A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication}},
author = {Jiang, Peng and Agrawal, Gagan},
booktitle = {Advances in Neural Information Processing Systems 31},
pages = {2525--2536},
year = {2018}
}

@techreport{Wang2019,
archivePrefix = {arXiv},
arxivId = {1810.08313},
author = {Wang, Jianyu and Joshi, Gauri},
eprint = {1810.08313},
file = {:mnt/01D35D8F61D34AA0/Learning/Research/Wang, Joshi - 2019 - Adaptive Communication Strategies to achieve the best error-runtime trade-off in Local-Update SGD.pdf:pdf},
title = {{Adaptive Communication Strategies to Achieve the Best Error-Runtime Trade-off in Local-Update SGD}},
year = {2018}
}

@incollection{Patel19,
title = {{Communication trade-offs for Local-SGD with large step size}},
author = {Dieuleveut, Aymeric and Patel, Kumar Kshitij},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {13579--13590},
year = {2019}
}

@article{Wang18,
Author = {Jianyu Wang and Gauri Joshi},
Title = {{Cooperative SGD: A Unified Framework for the Design and Analysis of Communication-Efficient SGD Algorithms}},
Year = {2018},
journal = {preprint arXiv:1808.07576}
}
@inproceedings{TaoLin19,
title={{Don't Use Large Mini-batches, Use Local SGD}},
author={Tao Lin and Sebastian U. Stich and Kumar Kshitij Patel and Martin Jaggi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=B1eyO1BFPr}
}

@inproceedings{Li2019,
title={{On the Convergence of FedAvg on Non-IID Data}},
author={Xiang Li and Kaixuan Huang and Wenhao Yang and Shusen Wang and Zhihua Zhang},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HJxNAnVtDS}
}

@inproceedings{Stich2018,
	title={{Local SGD Converges Fast and Communicates Little}},
	author={Sebastian U. Stich},
	booktitle={International Conference on Learning Representations},
	year={2019}
}
@article{Zhou18,
   title={{On the Convergence Properties of a K-step Averaging Stochastic Gradient Descent Algorithm for Nonconvex Optimization}},
   journal={Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence},
   author={Zhou, Fan and Cong, Guojing},
   year={2018}
}

@article{Yu18,
   title={{Parallel Restarted SGD with Faster Convergence and Less Communication: Demystifying Why Model Averaging Works for Deep Learning}},
   volume={33},
   journal={Proceedings of the AAAI Conference on Artificial Intelligence},
   author={Yu, Hao and Yang, Sen and Zhu, Shenghuo},
   pages={5693â€“5700},
   year={2019}
}

@book{Beck2017,
author = {Beck, A.},
title = {First-Order Methods in Optimization},
publisher = {Society for Industrial and Applied Mathematics},
year = {2017},
doi = {10.1137/1.9781611974997},
address = {Philadelphia, PA},
edition   = {},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611974997}
}


@Article{dalcin2011parallel,
  author    = {Dalcin, Lisandro D. and Paz, Rodrigo R. and Kler, Pablo A. and Cosimo, Alejandro},
  title     = {{Parallel distributed computing using Python}},
  journal   = {Advances in Water Resources},
  year      = {2011},
  volume    = {34},
  number    = {9},
  pages     = {1124--1139}
}


@article{stich19errorfeedback,
    title={{The Error-Feedback Framework: Better Rates for SGD with Delayed Gradients and Compressed Communication}},
    author={Sebastian U. Stich and Sai Praneeth Karimireddy},
    year={2019},
    journal={preprint arXiv:1909.05350}
}

@article{Reisizadeh19,
    title={{FedPAQ: A Communication-Efficient Federated Learning Method with Periodic Averaging and Quantization}},
    author={Amirhossein Reisizadeh and Aryan Mokhtari and Hamed Hassani and Ali Jadbabaie and Ramtin Pedarsani},
    year={2019},
    journal={preprint arXiv:1909.13014}
}

@article{WangTuor18,
  author    = {Shiqiang Wang and
               Tiffany Tuor and
               Theodoros Salonidis and
               Kin K. Leung and
               Christian Makaya and
               Ting He and
               Kevin Chan},
  title     = {{When Edge Meets Learning: Adaptive Control for Resource-Constrained
               Distributed Machine Learning}},
  journal   = {arXiv:1804.05271},
  year      = {2018},
  timestamp = {Mon, 10 Dec 2018 13:28:19 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1804-05271},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}







@InProceedings{yu2019linear,
  title = 	 {{On the Linear Speedup Analysis of Communication Efficient Momentum SGD for Distributed Non-Convex Optimization}},
  author = 	 {Yu, Hao and Jin, Rong and Yang, Sen},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  year = 	 {2019}
}

@article{wang2019slowmo,
    title={{SlowMo: Improving Communication-Efficient Distributed SGD with Slow Momentum}},
    author={Jianyu Wang and Vinayak Tantia and Nicolas Ballas and Michael Rabbat},
    year={2019},
    journal={preprint arXiv:1910.00643}
}

@article{liang2019variance,
    title={{Variance Reduced Local SGD with Lower Communication Complexity}},
    author={Xianfeng Liang and Shuheng Shen and Jingchang Liu and Zhen Pan and Enhong Chen and Yifei Cheng},
    year={2019},
    journal={preprint arXiv:1912.12844}
}

@article{karimireddy2019scaffold,
    title={{SCAFFOLD: Stochastic Controlled Averaging for On-Device Federated Learning}},
    author={Sai Praneeth Karimireddy and Satyen Kale and Mehryar Mohri and Sashank J. Reddi and Sebastian U. Stich and Ananda Theertha Suresh},
    year={2019},
    journal={preprint arXiv:1910.06378}
}

@article{xie2019local,
    title={{Local AdaAlter: Communication-Efficient Stochastic Gradient Descent with Adaptive Learning Rates}},
    author={Cong Xie and Oluwasanmi Koyejo and Indranil Gupta and Haibin Lin},
    year={2019},
    journal={preprint arXiv:1911.09030}
}

@article{sharma2019parallel,
    title={{Parallel Restarted SPIDER -- Communication Efficient Distributed Nonconvex Optimization with Optimal Computation Complexity}},
    author={Pranay Sharma and Prashant Khanduri and Saikiran Bulusu and Ketan Rajawat and Pramod K. Varshney},
    year={2019},
    journal={preprint arXiv:1912.06036}
}

@inproceedings{mishchenko2018delay,
  title={{A Delay-tolerant Proximal-Gradient Algorithm for Distributed Learning}},
  author={Mishchenko, Konstantin and Iutzeler, Franck and Malick, J{\'e}r{\^o}me and Amini, Massih-Reza},
  booktitle={International Conference on Machine Learning},
  pages={3584--3592},
  year={2018}
}

@article{mishchenko2019revisiting,
  title={{Revisiting Stochastic Extragradient}},
  author={Mishchenko, Konstantin and Kovalev, Dmitry and Shulgin, Egor and Richt{\'a}rik, Peter and Malitsky, Yura},
  journal = {to appear in the 23rd International Conference on Artificial
  Intelligence and Statistics (AISTATS)},
  year={2019}
}

@article{juditsky2011solving,
  title={{Solving variational Inequalities with Stochastic Mirror-Prox algorithm}},
  author={Juditsky, Anatoli and Nemirovski, Arkadi and Tauvel, Claire},
  journal={Stochastic Systems},
  volume={1},
  number={1},
  pages={17--58},
  year={2011}
}

@incollection{chavdarova2019reducing,
	title = {{Reducing Noise in GAN Training with Variance Reduced Extragradient}},
	author = {Chavdarova, Tatjana and Gidel, Gauthier and Fleuret, Fran\c{c}ois and Lacoste-Julien, Simon},
	booktitle = {Advances in Neural Information Processing Systems 32},
	pages = {391--401},
	year = {2019},
	publisher = {Curran Associates, Inc.}
}


@article{Sahu18,
    title={{Federated Optimization in Heterogeneous Networks}},
    author={Tian Li and Anit Kumar Sahu and Manzil Zaheer and Maziar Sanjabi and Ameet Talwalkar and Virginia Smith},
    year={2018},
    journal={preprint arXiv:1812.06127}
}


@article{HardRao18,
  author    = {Andrew Hard and
               Kanishka Rao and
               Rajiv Mathews and
               Fran{\c{c}}oise Beaufays and
               Sean Augenstein and
               Hubert Eichner and
               Chlo{\'{e}} Kiddon and
               Daniel Ramage},
  title     = {{Federated Learning for Mobile Keyboard Prediction}},
  journal   = {preprint arXiv:1811.03604},
  year      = {2018},
  timestamp = {Fri, 23 Nov 2018 12:43:51 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1811-03604},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{zhang2019distributed,
    title={{Distributed Optimization for Over-Parameterized Learning}},
    author={Chi Zhang and Qianxiao Li},
    year={2019},
    journal={preprint arXiv:1906.06205}
}

@inproceedings{Haddadpour2019local,
title = {{Local SGD with  Periodic Averaging: Tighter Analysis  and Adaptive Synchronization}},
author = {Haddadpour, Farzin and Kamani, Mohammad Mahdi and Mahdavi, Mehrdad and Cadambe, Viveck},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {11080--11092},
year = {2019}
}


@article{Haddadpour19FL,
    title={{On the Convergence of Local Descent Methods in Federated Learning}},
    author={Farzin Haddadpour and Mehrdad Mahdavi},
    year={2019},
    journal={preprint arXiv:1910.14425}
}

@InProceedings{MAML2017,
  author    = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  title     = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  booktitle = {International Conference on Machine Learning},
  year      = {2017},
}

@Article{FL2019variational,
  author  = {L. Corinzia and J. M. Buhmann},
  title   = {Variational federated multi-task learning},
  journal = {preprint arXiv:1906.06268},
  year    = {2019},
}

@InProceedings{Eichner2019semi-cyclicSGD_for_FL,
  author    = {H. Eichner and T. Koren and H. B. McMahan and N. Srebro and K. Talwar},
  title     = {Semi-cyclic stochastic gradient descent},
  booktitle = {International Conference on Machine Learning},
  year      = {2019},
}

@Article{Zhao2018FL-transfer-learn,
  author  = {Y. Zhao and M. Li and L. Lai and N. Suda and D. Civin and V. Chandra},
  title   = {Federated learning with non-iid data},
  journal = {preprint arXiv:1806.00582},
  year    = {2018},
}

@InProceedings{Khodak2019FLmeta,
  author  = {M. Khodak and M.-F. Balcan and A. Talwalkar},
  title   = {Adaptive gradient-based meta-learning methods},
  year    = {2019},
  booktitle = {Advances in Neural Information Processing Systems 32},
}

@unpublished{kha20,
  author  = "A. Khaled and O. Sebbouh and N. Loizou and R. M. Gower M. and P. {Richt\'{a}rik}",
  note = {arXiv:2006.11573},
  title   = {Unified Analysis of Stochastic Gradient Methods for Composite Convex and Smooth Optimization},
  year    = {2020},
}

@conference{gor202,
  author    = "E. Gorbunov  and F. Hanzely and P. {Richt\'{a}rik}",
  title     = "A Unified Theory of {SGD: Variance} Reduction, Sampling, Quantization and Coordinate Descent",
  booktitle ="Proc. of 23rd Int. Conf. Artificial Intelligence and Statistics (AISTATS)",
  year      = {2020},
}



@inproceedings{allen2017katyusha,
  title={Katyusha: The first direct acceleration of stochastic gradient methods},
  author={Allen-Zhu, Zeyuan},
  booktitle={Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing},
  pages={1200--1205},
  year={2017},
  organization={ACM}
}

@book{NesterovBook,
  title     = {Introductory lectures on convex optimization: a basic course},
  publisher = {Kluwer Academic Publishers},
  year      = {2004},
  author    = {Yurii Nesterov},
}

@InProceedings{IProx-SDCA,
  author    = {Zhao, Peilin and Zhang, Tong},
  title     = {Stochastic optimization with importance sampling for regularized loss minimization},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning, PMLR},
  year      = {2015},
  volume    = {37},
  pages     = {1--9},
  journal   = {The 32nd International Conference on Machine Learning},
}

@InProceedings{pmlr-v97-qian19b,
  title = 	 {{SGD}: General Analysis and Improved Rates},
  author = 	 {Gower, Robert Mansel and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richt{\'a}rik, Peter},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5200--5209},
  year = 	 {2019},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  url = 	 {http://proceedings.mlr.press/v97/qian19b.html},
}

@Article{AIDE,
  author        = {Reddi, Sashank J. and Kone\v{c}n\'{y}, Jakub and Richt\'{a}rik, Peter and P\'{o}czos, Barnab\'{a}s and Smola, Alex},
  title         = {{AIDE}: fast and communication efficient distributed optimization},
  journal       = {arXiv:1608.06879},
  year          = {2016},
}

@InProceedings{Dean2012,
  author    = {Jeffrey Dean and Greg Corrado and Rajat Monga and Kai Chen and Matthieu Devin and Mark Mao and Andrew Senior and Paul Tucker and Ke Yang and Quoc V Le and et al},
  title     = {Large scale distributed deep networks},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2012},
  pages     = {1223--1231},
}


@article{mishchenko2019stochastic,
  title={A stochastic decoupling method for minimizing the sum of smooth and non-smooth functions},
  author={Mishchenko, Konstantin and Richt{\'a}rik, Peter},
  journal={preprint arXiv:1905.11535},
  year={2019}
}

@InProceedings{gazagnadou2019optimal,
  title={Optimal mini-batch and step sizes for {SAGA}},
  author={Gazagnadou, Nidham and Gower, Robert M and Salmon, Joseph},
  booktitle={International Conference on Machine Learning},
  year={2019}
}

@Article{FL-big,
  author  = {Kairouz, Peter and McMahan, H. Brendan and et al},
  title   = {Advances and Open Problems in Federated Learning},
  journal = {preprint arXiv:1912.04977v1},
  year    = {2019},
}

@Article{FL-FedProx,
  author  = {Li, Tian Li and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  title   = {Federated Optimization for Heterogeneous Networks},
  journal = {preprint arXiv:1812.06127},
  year    = {2018},
}

% and Avent, Brendan and Bellet, Aur\'{e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Keith and Charles, Zachary and Cormode, Graham and Cummings, Rachel and Dâ€™Oliveira, Rafael G.L. and El Rouayheb, Salim and Evans, David and Gardner, Josh and Gasc\'{o}n, Adri\`{a} and Ghazi, Badih and Gibbons, Phillip B. and Garrett, Zachary et al

@InCollection{NIPS2017_FL-multitask,
  author    = {Smith, Virginia and Chiang, Chao-Kai and Sanjabi, Maziar and Talwalkar, Ameet S},
  title     = {Federated Multi-Task Learning},
  booktitle = {Advances in Neural Information Processing Systems 30},
  publisher = {Curran Associates, Inc.},
  year      = {2017},
  editor    = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {4424--4434},
  url       = {http://papers.nips.cc/paper/7029-federated-multi-task-learning.pdf},
}

@Article{FL_survey_2019,
  author  = {Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia},
  title   = {Federated learning: challenges, methods, and future directions},
  journal = {preprint arXiv:1908.07873},
  year    = {2019},
}



@InProceedings{localSGD-AISTATS2020,
  author    = {Khaled, Ahmed and Mishchenko, Konstantin and Richt\'{a}rik, Peter},
  title     = {Tighter theory for local {SGD} on identical and heterogeneous data},
  booktitle = {The 23rd International Conference on Artificial Intelligence and Statistics (AISTATS 2020)},
  year      = {2020},
}

@InProceedings{localSGD,
  author    = {Khaled, Ahmed and Mishchenko, Konstantin and Richt\'{a}rik, Peter},
  title     = {Better communication complexity for local {SGD}},
  booktitle = {NeurIPS Workshop on Federated Learning for Data Privacy and Confidentiality},
  year      = {2019},
}



@InProceedings{localSGD-Stich,
  author    = {Stich, Sebastian U.},
  title     = {Local {SGD} Converges Fast and Communicates Little},
  booktitle = {International Conference on Learning Representations},
  year      = {2020},
}



@Article{FedAvg2016,
  author  = {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Ag\"{u}era y Arcas, Blaise},
  title   = {Federated Learning of Deep Networks using Model Averaging},
  journal = {preprint arXiv:1602.05629},
  year    = {2016},
}

@Article{FEDOPT,
  author        = {Kone\v{c}n\'{y}, Jakub and McMahan, H. Brendan and Ramage, Daniel and Richt\'{a}rik, Peter},
  title         = {Federated optimization: distributed machine learning for on-device intelligence},
  journal       = {preprint arXiv:1610.02527},
  year          = {2016},
}





@inproceedings{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={315--323},
  year={2013}
}

@article{wu2019federated,
  title={Federated Variance-Reduced Stochastic Gradient Descent with Robustness to Byzantine Attacks},
  author={Wu, Zhaoxian and Ling, Qing and Chen, Tianyi and Giannakis, Georgios B},
  journal={preprint arXiv:1912.12716},
  year={2019}
}

@article{liang2019variance,
  title={Variance Reduced Local {SGD} with Lower Communication Complexity},
  author={Liang, Xianfeng and Shen, Shuheng and Liu, Jingchang and Pan, Zhen and Chen, Enhong and Cheng, Yifei},
  journal={preprint arXiv:1912.12844},
  year={2019}
}

@article{karimireddy2019scaffold,
  title={{SCAFFOLD:} Stochastic controlled averaging for on-device federated learning},
  author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank J and Stich, Sebastian U and Suresh, Ananda Theertha},
  journal={preprint arXiv:1910.06378},
  year={2019}
}

@inproceedings{hofmann2015variance,
  title={Variance reduced stochastic gradient descent with neighbors},
  author={Hofmann, Thomas and Lucchi, Aurelien and Lacoste-Julien, Simon and McWilliams, Brian},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2305--2313},
  year={2015}
}



@Article{JacSketch,
  author        = {Gower, Robert Mansel and Richt\'{a}rik, Peter and Bach, Francis},
  title         = {Stochastic quasi-gradient methods: variance reduction via {J}acobian sketching},
  journal       = {preprint arXiv:1805.02632},
  year          = {2018},
}

@InProceedings{qian2019saga,
  author    = {Qian, Xun and Qu, Zheng and Richt\'{a}rik, Peter},
  title     = {{SAGA} with arbitrary sampling},
  booktitle = {The 36th International Conference on Machine Learning},
  year      = {2019},
}



@InProceedings{kovalev2019don,
  author    = {Kovalev, Dmitry and Horv\'{a}th, Samuel and Richt\'{a}rik, Peter},
  title     = {Donâ€™t jump through hoops and remove those loops: {SVRG} and {K}atyusha are better without the outer loop},
  booktitle = {Proceedings of the 31st International Conference on Algorithmic Learning Theory},
  year      = {2020},
}



@article{ma2017distributed,
  title={Distributed optimization with arbitrary local solvers},
  author={Ma, Chenxin and Kone{\v{c}}n{\'y}, Jakub and Jaggi, Martin and Smith, Virginia and Jordan, Michael I and Richt{\'a}rik, Peter and Tak{\'a}{\v{c}}, Martin},
  journal={Optimization Methods and Software},
  volume={32},
  number={4},
  pages={813--848},
  year={2017},
  publisher={Taylor \& Francis}
}
@article{haddadpour2019convergence,
  title={On the Convergence of Local Descent Methods in Federated Learning},
  author={Haddadpour, Farzin and Mahdavi, Mehrdad},
  journal={preprint arXiv:1910.14425},
  year={2019}
}



